{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Preprocesamiento de los comentarios para la primera fase de fine-tuning y entrenamiento del modelo\n",
    "En esta primera fase, una vez extraídos los bodies de todos los issues/PRs de los repositorios listados junto con 2 comentarios de 250 del total de issues de cada repositorio, se prepararán todos los datos para ser utilizados en el entrenamiento y ajuste fino del modelo que se utilizará.\n",
    "\n",
    "Se seguirán prácticamente los mismos pasos vistos en *c_preparing_data_for_statistics_and_ML* pero con varias diferencias claves que existen entre los modelos BERT que se utilizarán ahora y los modelos de clasificación presentados con anterioridad (notebooks de GVTIA).\n",
    "\n",
    "Para Transformers funciona mejor un preprocesado mínimo y dejar la segmentación al propio tokenizador del modelo, a continuación se muestra que procedimientos similares a los anteriores se mantendrán y cuáles se evitarán:\n",
    "\n",
    "## Se mantendrá:\n",
    "- Normalización de espacios/saltos de línea\n",
    "- Eliminación de caracteres de control raros o poco usuales\n",
    "- Se conservará el uso de mayúsculas y minúsculas, signos, números, URLs, nombres propios de vulnerabilidades o bugs (CVE-XXXX-YYYY), rutas (/etc/...), código entre backticks (`return salida`), nombres de APIs.\n",
    "- Se definirá una longitud máxima de tokens por comentario o el uso de *sliding window* si el texto es muy largo.\n",
    "\n",
    "## Se omitirá:\n",
    "- Pasar todo el texto a minúsculas, los modelos RoBERTa/DistilRoBERTa que se utilizarán utilizan mayúsculas y minúsculas.\n",
    "- Eliminar la puntuación y stopwords.\n",
    "- Stemming / lematización.\n",
    "- Normalizaciones agresivas de URLs/código -> se pierde señal técnica."
   ],
   "id": "50b7964ea9a2b2da"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Para evitar tener que ejecutar el código al completo del notebook cada vez que quiera hacer algo, se definirá un patrón \"run-once\" para evitar crear los dataframes, bases de datos, ejecución de scripts, etc. si estos ya existen.",
   "id": "8cb3070784e58ee2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pathlib import Path\n",
    "import subprocess, sys, json, os\n",
    "\n",
    "FORCE = False   # True si se quiere rehacer una fase\n",
    "\n",
    "def run_once(out_path, cmd_list):\n",
    "    out = Path(out_path)\n",
    "\n",
    "    if (out.exists() and not FORCE):\n",
    "        print(f\"Ya existe: {out}. Saltando.\")\n",
    "        return\n",
    "\n",
    "    out.parent.mkdir(parents=True, exist_ok=True)\n",
    "    print(\"Ejecutando:\", \" \".join(cmd_list))\n",
    "\n",
    "    subprocess.run(cmd_list, check=True)\n",
    "    print(f\"Listo: {out}\")"
   ],
   "id": "10699e6e330bea7e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Una vez explicado esto, se comenzará con el preprocesado de todos los comentarios extraídos de GitHub, comenzando como se ha visto ya en diversas ocasiones, con cargar el documento (.csv) en un dataframe de pandas para su uso y manipulación.\n",
    "\n",
    "En este caso, se cuenta con 2 documentos:\n",
    "- **gh_bodys_lastyear.csv**. Archivo que contiene los bodies (comentario principal) de todos los Issues/PRs en el último año de los repositorios listados para la extracción de comentarios.\n",
    "- **gh_comments_lastyear.csv**. Archivo que contiene los 2 primeros comentarios de cada Issue/PR de 250 Issues/PRs por repositorio (500 comentarios por repo), en gran parte de los casos serán las respuestas aportadas por usuarios al body del documento anterior.\n",
    "\n",
    "En este caso, como se cuenta con 2 documentos lo que se hará es crear 2 dataframes, uno con cada documento, para a continuación unirlos con la función `concat()` de pandas y ordenarlos según el id del Issue/PR para la clara visualización y mantener una estructura coherente entre cuerpo principal y comentarios asociados."
   ],
   "id": "decd1abca07ad89a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T10:58:47.711186Z",
     "start_time": "2025-08-27T10:58:43.021047Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ruta de los archivos\n",
    "path_gh_bodys = \"../data/gh_comments/train-fine_tuning/gh_bodys_lastyear.csv\"\n",
    "path_gh_comments = \"../data/gh_comments/train-fine_tuning/gh_comments_lastyear.csv\"\n",
    "\n",
    "# Carga de los archivos en DataFrames\n",
    "df_bodys = pd.read_csv(path_gh_bodys)\n",
    "df_comms = pd.read_csv(path_gh_comments)"
   ],
   "id": "b0b67a9537d80fe3",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T10:04:12.827775Z",
     "start_time": "2025-08-26T10:04:12.816636Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(df_bodys.columns)\n",
    "print(df_comms.columns)"
   ],
   "id": "2ecea4e9efb540d5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['repo', 'is_pr', 'issue_number', 'comment_type', 'comment_id',\n",
      "       'comment_created_at', 'comment_author', 'text', 'comment_url',\n",
      "       'context_id', 'container_title', 'container_state', 'container_url',\n",
      "       'container_created_at', 'container_updated_at', 'container_labels'],\n",
      "      dtype='object')\n",
      "Index(['kubernetes/kubernetes', 'False', '133680', 'issue_comment',\n",
      "       'github_issuecomment_IC_kwDOAToIks6_4TOW', '2025-08-25T07:51:17Z',\n",
      "       'k8s-ci-robot',\n",
      "       'This issue is currently awaiting triage.\\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the triage/accepted label and provide further guidance.\\nThe triage/accepted label can be added by org members by writing /triage accepted in a comment.\\n\\nInstructions for interacting with me using PR comments are available here.  If you have questions or suggestions related to my behavior, please file an issue against the kubernetes-sigs/prow repository.',\n",
      "       'https://github.com/kubernetes/kubernetes/issues/133680#issuecomment-3219207062',\n",
      "       'kubernetes/kubernetes#issue:133680',\n",
      "       'Add new imagePullPolicy mode IfNewerNotPresent, that pulls image if image ID (ref/digest) on remote is not matching.',\n",
      "       'OPEN', 'https://github.com/kubernetes/kubernetes/issues/133680',\n",
      "       '2025-08-25T07:51:08Z', '2025-08-26T08:18:05Z',\n",
      "       'sig/node;kind/feature;needs-triage'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Se ha cometido un error en la escritura de la cabecera de los comentarios por escribir siempre en el mismo documento y borrar su contenido en vez de eliminar el documento antes de comenzar con una nueva extracción. Vamos a tratar de repararlo sin tener que volver a realizar todo el proceso de extracción.",
   "id": "928929f1de768f4e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T10:51:25.921409Z",
     "start_time": "2025-08-26T10:51:23.468128Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "path_gh_bodys = Path(path_gh_bodys)\n",
    "path_gh_comments = Path(path_gh_comments)\n",
    "\n",
    "EXPECTED_COLS = [\n",
    "    'repo','is_pr','issue_number','comment_type','comment_id','comment_created_at','comment_author',\n",
    "    'text','comment_url','context_id','container_title','container_state','container_url',\n",
    "    'container_created_at','container_updated_at','container_labels'\n",
    "]\n",
    "\n",
    "def read_with_header_fix(p: Path) -> pd.DataFrame:\n",
    "    # Se lee 1 fila para inspeccionar columnas\n",
    "    probe = pd.read_csv(p, nrows=1)\n",
    "    if list(probe.columns) == EXPECTED_COLS:\n",
    "        return pd.read_csv(p)\n",
    "    # Si no coincide, reinterpretamos: no hay cabecera -> header=None + names=EXPECTED_COLS\n",
    "    return pd.read_csv(p, header=None, names=EXPECTED_COLS)\n",
    "\n",
    "df_bodys = read_with_header_fix(path_gh_bodys)\n",
    "df_comms = read_with_header_fix(path_gh_comments)\n",
    "\n",
    "# Se unen ambos DataFrames\n",
    "df_gh = pd.concat([df_bodys, df_comms], ignore_index=True)\n",
    "\n",
    "# Tipos y ordenación\n",
    "df_gh['comment_created_at'] = pd.to_datetime(df_gh['comment_created_at'], errors='coerce', utc=True)\n",
    "df_gh.loc[df_gh['comment_created_at'].isna(), 'comment_created_at'] = pd.to_datetime(df_gh['container_created_at'], errors='coerce', utc=True)\n",
    "\n",
    "order_map = {'issue_body':0, 'pr_body':0} # Bodies primero -> coherencia\n",
    "df_gh['order'] = df_gh['comment_type'].map(order_map).fillna(1).astype(int)\n",
    "\n",
    "df_gh = df_gh.sort_values(by=['repo','issue_number','order','comment_created_at','comment_id'], kind='mergesort').drop(columns=['order'])\n",
    "\n",
    "# Normalizar booleano -> OPCIONAL\n",
    "df_gh['is_pr'] = df_gh['is_pr'].astype(str).str.lower().map({'true':True, 'false':False})"
   ],
   "id": "3034471a4637f6e8",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T10:57:45.765197Z",
     "start_time": "2025-08-27T10:57:45.729456Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Muestra para comprobar que se ha ejecutado correctamente\n",
    "print(df_bodys.columns)\n",
    "print(df_comms.columns)\n",
    "\n",
    "df_gh.head(10).T\n",
    "print(len(df_gh))"
   ],
   "id": "f7895ae432b8ec3b",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_bodys' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Muestra para comprobar que se ha ejecutado correctamente\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mdf_bodys\u001B[49m\u001B[38;5;241m.\u001B[39mcolumns)\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(df_comms\u001B[38;5;241m.\u001B[39mcolumns)\n\u001B[0;32m      5\u001B[0m df_gh\u001B[38;5;241m.\u001B[39mhead(\u001B[38;5;241m10\u001B[39m)\u001B[38;5;241m.\u001B[39mT\n",
      "\u001B[1;31mNameError\u001B[0m: name 'df_bodys' is not defined"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Ahora sí están todos los comentarios bien ordenados. Antes de comenzar con el preprocesado vamos a guardar el dataframe en una base de datos.",
   "id": "65043b29d28ed666"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T10:56:13.432374Z",
     "start_time": "2025-08-27T10:56:13.166017Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sqlite3\n",
    "db_gh = \"../data/gh_comments/train-fine_tuning/gh_dataset_lastyear.db\"\n",
    "con = sqlite3.connect(db_gh)\n",
    "df_gh.to_sql('gh_dataset', con, if_exists='replace', index=False)\n",
    "con.close()"
   ],
   "id": "de15d1140cc5ed4",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_gh' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 4\u001B[0m\n\u001B[0;32m      2\u001B[0m db_gh \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m../data/gh_comments/train-fine_tuning/gh_dataset_lastyear.db\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m      3\u001B[0m con \u001B[38;5;241m=\u001B[39m sqlite3\u001B[38;5;241m.\u001B[39mconnect(db_gh)\n\u001B[1;32m----> 4\u001B[0m \u001B[43mdf_gh\u001B[49m\u001B[38;5;241m.\u001B[39mto_sql(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgh_dataset\u001B[39m\u001B[38;5;124m'\u001B[39m, con, if_exists\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mreplace\u001B[39m\u001B[38;5;124m'\u001B[39m, index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m      5\u001B[0m con\u001B[38;5;241m.\u001B[39mclose()\n",
      "\u001B[1;31mNameError\u001B[0m: name 'df_gh' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Ahora sí se procederá al procesamiento del dataset para dejarlo preparado para el modelo BERT que se utilizará, RoBERTa o DistilRoBERTa. Este proceso se va a definir en una serie de scripts .py, cada uno con el objetivo de realizar una tarea para su reutilización en otros puntos del proyecto (cuando se haga el de reddit, u otros comentarios de github) de forma que estos sean agnósticos al sistema del que se extraen los comentarios que serán utilizados por el modelo.\n",
    "\n",
    "Del mismo modo, tras el procesamiento de los datos, el resultado del procesado será almacenado en archivos `.parquet` por su ligereza y agilidad a la hora de ser manipulados y consumidos por modelos BERT. Las principales ventajas de este formato son:\n",
    "- Más pequeño: compresión por columna, pesa de 2 a 5 veces menos que un `.csv`\n",
    "- Más rápido: lee solo las columnas que se necesitan (\"column pruning\") y aplica vectorización.\n",
    "- Conserva tipos: fechas, booleanos, enteros \"nullable\", etc. (`.csv`los pierde)\n",
    "- Esquema: guarda el _schema_ dentro del archivo -> menos sorpresas al cargarlo\n",
    "\n",
    "Por estas características el formato es el preferido para pipelines de datos/ML. En este caso la estructura que se utilizará será:\n",
    "- `merged.parquet` = será el dataset completo tras ingesta y normalización ligera.\n",
    "- `split_train.parquet`, `split_dev.parquet`, `split_test.parquet` = particiones del dataset ya divididas, listas para su tokenización.\n",
    "\n",
    "#### Ventajas frente a CSV\n",
    "- Evita problemas de comas y saltos de línea\n",
    "- Mantiene las fechas (`created_at`), booleanos (`is_pr`) y enteros sin perder el tipo.\n",
    "- Carga solo las columnas necesarias -> menor uso de RAM y tiempo de ejecución."
   ],
   "id": "17b3a603ab9a704c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
