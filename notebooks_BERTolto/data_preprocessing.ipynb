{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "R# Preprocesamiento de los comentarios para la primera fase de fine-tuning y entrenamiento del modelo\n",
    "En esta primera fase, una vez extraídos los bodies de todos los issues/PRs de los repositorios listados junto con 2 comentarios de 250 del total de issues de cada repositorio, se prepararán todos los datos para ser utilizados en el entrenamiento y ajuste fino del modelo que se utilizará.\n",
    "\n",
    "Se seguirán prácticamente los mismos pasos vistos en *c_preparing_data_for_statistics_and_ML* pero con varias diferencias claves que existen entre los modelos BERT que se utilizarán ahora y los modelos de clasificación presentados con anterioridad (notebooks de GVTIA).\n",
    "\n",
    "Para Transformers funciona mejor un preprocesado mínimo y dejar la segmentación al propio tokenizador del modelo, a continuación se muestra que procedimientos similares a los anteriores se mantendrán y cuáles se evitarán:\n",
    "\n",
    "## Se mantendrá:\n",
    "- Normalización de espacios/saltos de línea\n",
    "- Eliminación de caracteres de control raros o poco usuales\n",
    "- Se conservará el uso de mayúsculas y minúsculas, signos, números, URLs, nombres propios de vulnerabilidades o bugs (CVE-XXXX-YYYY), rutas (/etc/...), código entre backticks (`return salida`), nombres de APIs.\n",
    "- Se definirá una longitud máxima de tokens por comentario o el uso de *sliding window* si el texto es muy largo.\n",
    "\n",
    "## Se omitirá:\n",
    "- Pasar todo el texto a minúsculas, los modelos RoBERTa/DistilRoBERTa que se utilizarán utilizan mayúsculas y minúsculas.\n",
    "- Eliminar la puntuación y stopwords.\n",
    "- Stemming / lematización.\n",
    "- Normalizaciones agresivas de URLs/código -> se pierde señal técnica."
   ],
   "id": "50b7964ea9a2b2da"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Una vez explicado esto, se comenzará con el preprocesado de todos los comentarios extraídos de GitHub, comenzando como se ha visto ya en diversas ocasiones, con cargar el documento (.csv) en un dataframe de pandas para su uso y manipulación.\n",
    "\n",
    "En este caso, se cuenta con 2 documentos:\n",
    "- **gh_bodys_lastyear.csv**. Archivo que contiene los bodies (comentario principal) de todos los Issues/PRs en el último año de los repositorios listados para la extracción de comentarios.\n",
    "- **gh_comments_lastyear.csv**. Archivo que contiene los 2 primeros comentarios de cada Issue/PR de 250 Issues/PRs por repositorio (500 comentarios por repo), en gran parte de los casos serán las respuestas aportadas por usuarios al body del documento anterior.\n",
    "\n",
    "En este caso, como se cuenta con 2 documentos lo que se hará es crear 2 dataframes, uno con cada documento, para a continuación unirlos con la función `concat()` de pandas y ordenarlos según el id del Issue/PR para la clara visualización y mantener una estructura coherente entre cuerpo principal y comentarios asociados."
   ],
   "id": "decd1abca07ad89a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T20:28:59.177215Z",
     "start_time": "2025-08-27T20:28:37.461696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cargamos el archivo settings_bert.py que contiene todos los imports necesarios para la correcta ejecución del notebook y no tener que importarlos en cada celda de código\n",
    "%run -i ../settings_bert.py"
   ],
   "id": "3ed28d7ff902c74f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T20:29:03.640986Z",
     "start_time": "2025-08-27T20:28:59.191216Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Ruta de los archivos\n",
    "path_gh_bodys = \"../data/gh_comments/train-fine_tuning/gh_bodys_lastyear.csv\"\n",
    "path_gh_comments = \"../data/gh_comments/train-fine_tuning/gh_comments_lastyear.csv\"\n",
    "\n",
    "# Carga de los archivos en DataFrames\n",
    "df_bodys = pd.read_csv(path_gh_bodys)\n",
    "df_comms = pd.read_csv(path_gh_comments)"
   ],
   "id": "b0b67a9537d80fe3",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T20:29:03.672988Z",
     "start_time": "2025-08-27T20:29:03.657991Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(df_bodys.columns)\n",
    "print(df_comms.columns)"
   ],
   "id": "2ecea4e9efb540d5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['repo', 'is_pr', 'issue_number', 'comment_type', 'comment_id',\n",
      "       'comment_created_at', 'comment_author', 'text', 'comment_url',\n",
      "       'context_id', 'container_title', 'container_state', 'container_url',\n",
      "       'container_created_at', 'container_updated_at', 'container_labels'],\n",
      "      dtype='object')\n",
      "Index(['kubernetes/kubernetes', 'False', '133680', 'issue_comment',\n",
      "       'github_issuecomment_IC_kwDOAToIks6_4TOW', '2025-08-25T07:51:17Z',\n",
      "       'k8s-ci-robot',\n",
      "       'This issue is currently awaiting triage.\\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the triage/accepted label and provide further guidance.\\nThe triage/accepted label can be added by org members by writing /triage accepted in a comment.\\n\\nInstructions for interacting with me using PR comments are available here.  If you have questions or suggestions related to my behavior, please file an issue against the kubernetes-sigs/prow repository.',\n",
      "       'https://github.com/kubernetes/kubernetes/issues/133680#issuecomment-3219207062',\n",
      "       'kubernetes/kubernetes#issue:133680',\n",
      "       'Add new imagePullPolicy mode IfNewerNotPresent, that pulls image if image ID (ref/digest) on remote is not matching.',\n",
      "       'OPEN', 'https://github.com/kubernetes/kubernetes/issues/133680',\n",
      "       '2025-08-25T07:51:08Z', '2025-08-26T08:18:05Z',\n",
      "       'sig/node;kind/feature;needs-triage'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Se ha cometido un error en la escritura de la cabecera de los comentarios por escribir siempre en el mismo documento y borrar su contenido en vez de eliminar el documento antes de comenzar con una nueva extracción. Vamos a tratar de repararlo sin tener que volver a realizar todo el proceso de extracción.",
   "id": "928929f1de768f4e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T20:29:08.668543Z",
     "start_time": "2025-08-27T20:29:03.718208Z"
    }
   },
   "cell_type": "code",
   "source": [
    "path_gh_bodys = Path(path_gh_bodys)\n",
    "path_gh_comments = Path(path_gh_comments)\n",
    "\n",
    "EXPECTED_COLS = [\n",
    "    'repo','is_pr','issue_number','comment_type','comment_id','comment_created_at','comment_author',\n",
    "    'text','comment_url','context_id','container_title','container_state','container_url',\n",
    "    'container_created_at','container_updated_at','container_labels'\n",
    "]\n",
    "\n",
    "def read_with_header_fix(p: Path) -> pd.DataFrame:\n",
    "    # Se lee 1 fila para inspeccionar columnas\n",
    "    probe = pd.read_csv(p, nrows=1)\n",
    "    if list(probe.columns) == EXPECTED_COLS:\n",
    "        return pd.read_csv(p)\n",
    "    # Si no coincide, reinterpretamos: no hay cabecera -> header=None + names=EXPECTED_COLS\n",
    "    return pd.read_csv(p, header=None, names=EXPECTED_COLS)\n",
    "\n",
    "df_bodys = read_with_header_fix(path_gh_bodys)\n",
    "df_comms = read_with_header_fix(path_gh_comments)\n",
    "\n",
    "# Se unen ambos DataFrames\n",
    "df_gh = pd.concat([df_bodys, df_comms], ignore_index=True)\n",
    "\n",
    "# Tipos y ordenación\n",
    "df_gh['comment_created_at'] = pd.to_datetime(df_gh['comment_created_at'], errors='coerce', utc=True)\n",
    "df_gh.loc[df_gh['comment_created_at'].isna(), 'comment_created_at'] = pd.to_datetime(df_gh['container_created_at'], errors='coerce', utc=True)\n",
    "\n",
    "order_map = {'issue_body':0, 'pr_body':0} # Bodies primero -> coherencia\n",
    "df_gh['order'] = df_gh['comment_type'].map(order_map).fillna(1).astype(int)\n",
    "\n",
    "df_gh = df_gh.sort_values(by=['repo','issue_number','order','comment_created_at','comment_id'], kind='mergesort').drop(columns=['order'])\n",
    "\n",
    "# Normalizar booleano -> OPCIONAL\n",
    "df_gh['is_pr'] = df_gh['is_pr'].astype(str).str.lower().map({'true':True, 'false':False})"
   ],
   "id": "3034471a4637f6e8",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T20:29:08.700556Z",
     "start_time": "2025-08-27T20:29:08.684552Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Muestra para comprobar que se ha ejecutado correctamente\n",
    "print(df_bodys.columns)\n",
    "print(df_comms.columns)\n",
    "\n",
    "df_gh.head(10).T\n",
    "print(len(df_gh))"
   ],
   "id": "f7895ae432b8ec3b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['repo', 'is_pr', 'issue_number', 'comment_type', 'comment_id',\n",
      "       'comment_created_at', 'comment_author', 'text', 'comment_url',\n",
      "       'context_id', 'container_title', 'container_state', 'container_url',\n",
      "       'container_created_at', 'container_updated_at', 'container_labels'],\n",
      "      dtype='object')\n",
      "Index(['repo', 'is_pr', 'issue_number', 'comment_type', 'comment_id',\n",
      "       'comment_created_at', 'comment_author', 'text', 'comment_url',\n",
      "       'context_id', 'container_title', 'container_state', 'container_url',\n",
      "       'container_created_at', 'container_updated_at', 'container_labels'],\n",
      "      dtype='object')\n",
      "108078\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Ahora sí están todos los comentarios bien ordenados. Antes de comenzar con el preprocesado vamos a guardar el dataframe en una base de datos.",
   "id": "65043b29d28ed666"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T20:29:08.747556Z",
     "start_time": "2025-08-27T20:29:08.736559Z"
    }
   },
   "cell_type": "code",
   "source": "print(len(df_gh))",
   "id": "b425f5858db09200",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108078\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T20:29:16.558398Z",
     "start_time": "2025-08-27T20:29:08.765560Z"
    }
   },
   "cell_type": "code",
   "source": [
    "db_gh = \"../data/gh_comments/train-fine_tuning/gh_dataset_lastyear.db\"\n",
    "con = sqlite3.connect(db_gh)\n",
    "df_gh.to_sql('gh_dataset', con, if_exists='replace', index=False)\n",
    "con.close()"
   ],
   "id": "de15d1140cc5ed4",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T20:29:16.978147Z",
     "start_time": "2025-08-27T20:29:16.567984Z"
    }
   },
   "cell_type": "code",
   "source": [
    "db_gh = pathlib.Path(\"../data/gh_comments/train-fine_tuning/gh_dataset_lastyear.db\")\n",
    "with sqlite3.connect(db_gh) as con:\n",
    "    n_rows = con.execute(\"SELECT COUNT(*) FROM gh_dataset\").fetchone()[0]\n",
    "print(f\"Filas en la tabla: {n_rows:,}\")\n",
    "\n",
    "with sqlite3.connect(db_gh) as con:\n",
    "    sample = con.execute(\"\"\"\n",
    "        SELECT * FROM gh_dataset\n",
    "        ORDER BY RANDOM()\n",
    "        LIMIT 10\n",
    "    \"\"\").fetchall()\n",
    "sample[:3]   # imprime tres filas de ejemplo\n"
   ],
   "id": "bbbf70e27843042a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas en la tabla: 108,078\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('pytorch/pytorch',\n",
       "  1,\n",
       "  145759,\n",
       "  'pr_body',\n",
       "  'github_prbody_pytorch/pytorch#145759',\n",
       "  '2025-01-27 19:26:04+00:00',\n",
       "  'c-p-i-o',\n",
       "  'Stack from ghstack (oldest at bottom):\\n\\n-> #145759\\n#145757\\n#145756\\n\\nSummary:\\nAllocations typically happen as a power of 2 anyway.\\nChange the default alloc size to 4096 so eek out a bit more perf.\\nTest:\\nunit tests\\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k',\n",
       "  'https://github.com/pytorch/pytorch/pull/145759',\n",
       "  'pytorch/pytorch#pr:145759',\n",
       "  '[chore][ez] change alloc buffer size from 4000 to 4096',\n",
       "  'CLOSED',\n",
       "  'https://github.com/pytorch/pytorch/pull/145759',\n",
       "  '2025-01-27T19:26:04Z',\n",
       "  '2025-02-28T02:11:43Z',\n",
       "  'oncall: distributed;Merged;ciflow/trunk;release notes: distributed (c10d)'),\n",
       " ('grafana/grafana',\n",
       "  1,\n",
       "  94857,\n",
       "  'pr_body',\n",
       "  'github_prbody_grafana/grafana#94857',\n",
       "  '2024-10-17 10:41:08+00:00',\n",
       "  'aangelisc',\n",
       "  'Supporting publishing of security versions',\n",
       "  'https://github.com/grafana/grafana/pull/94857',\n",
       "  'grafana/grafana#pr:94857',\n",
       "  'CI: Support additional version format in NPM publishing',\n",
       "  'CLOSED',\n",
       "  'https://github.com/grafana/grafana/pull/94857',\n",
       "  '2024-10-17T10:41:08Z',\n",
       "  '2024-10-17T13:05:52Z',\n",
       "  'area/backend;type/ci;no-changelog'),\n",
       " ('pytorch/pytorch',\n",
       "  0,\n",
       "  151540,\n",
       "  'issue_body',\n",
       "  'github_issuebody_pytorch/pytorch#151540',\n",
       "  '2025-04-17 09:42:33+00:00',\n",
       "  'pytorch-bot',\n",
       "  'Platforms: asan, linux, mac, macos, rocm, slow\\nThis test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs.\\nOver the past 3 hours, it has been determined flaky in 23 workflow(s) with 46 failures and 23 successes.\\nDebugging instructions (after clicking on the recent samples link):\\nDO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.\\nTo find relevant log snippets:\\n\\nClick on the workflow logs linked above\\nClick on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.\\nGrep for test_remove_noop_view_dtype_cpu\\nThere should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.\\n\\nTest file path: inductor/test_compile_subprocess.py\\ncc @clee2000 @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv',\n",
       "  'https://github.com/pytorch/pytorch/issues/151540',\n",
       "  'pytorch/pytorch#issue:151540',\n",
       "  'DISABLED test_remove_noop_view_dtype_cpu (__main__.CpuTests)',\n",
       "  'OPEN',\n",
       "  'https://github.com/pytorch/pytorch/issues/151540',\n",
       "  '2025-04-17T09:42:33Z',\n",
       "  '2025-08-14T01:02:06Z',\n",
       "  'triaged;module: flaky-tests;skipped;module: fx')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Ahora sí se procederá al procesamiento del dataset para dejarlo preparado para el modelo BERT que se utilizará, RoBERTa o DistilRoBERTa. Este proceso se va a definir en una serie de scripts .py, cada uno con el objetivo de realizar una tarea para su reutilización en otros puntos del proyecto (cuando se haga el de reddit, u otros comentarios de github) de forma que estos sean agnósticos al sistema del que se extraen los comentarios que serán utilizados por el modelo.\n",
    "\n",
    "Del mismo modo, tras el procesamiento de los datos, el resultado del procesado será almacenado en archivos `.parquet` por su ligereza y agilidad a la hora de ser manipulados y consumidos por modelos BERT. Las principales ventajas de este formato son:\n",
    "- Más pequeño: compresión por columna, pesa de 2 a 5 veces menos que un `.csv`\n",
    "- Más rápido: lee solo las columnas que se necesitan (\"column pruning\") y aplica vectorización.\n",
    "- Conserva tipos: fechas, booleanos, enteros \"nullable\", etc. (`.csv`los pierde)\n",
    "- Esquema: guarda el _schema_ dentro del archivo -> menos sorpresas al cargarlo\n",
    "\n",
    "Por estas características el formato es el preferido para pipelines de datos/ML. En este caso la estructura que se utilizará será:\n",
    "- `merged.parquet` = será el dataset completo tras ingesta y normalización ligera.\n",
    "- `split_train.parquet`, `split_dev.parquet`, `split_test.parquet` = particiones del dataset ya divididas, listas para su tokenización.\n",
    "\n",
    "#### Ventajas frente a CSV\n",
    "- Evita problemas de comas y saltos de línea\n",
    "- Mantiene las fechas (`created_at`), booleanos (`is_pr`) y enteros sin perder el tipo.\n",
    "- Carga solo las columnas necesarias -> menor uso de RAM y tiempo de ejecución."
   ],
   "id": "17b3a603ab9a704c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Ejecución del preprocesado de datos\n",
    "Se han definido varios scripts, cada uno con una función en el preprocesado de datos para modelos BERT, a  los cuáles se les llamará desde este notebook con los argumentos correspondientes para realizar este proceso.\n",
    "\n",
    "Las funciones reutilizables para el pipeline están definidas en `prep_utils.py`. Normaliza texto \"ligero\", mapea columnas heterogéneas (GH/Reddit) al esquema core (id, text, label, source, created_at, context_id), lee CSV/SQLite y guarda Parquet.\n",
    "El objetivo principal de estas funciones es su utilización cuando se desea agnosticismo de fuente (Github/Reddit) y un preprocesado mínimo ideal para Transformers."
   ],
   "id": "3b3d14442f2d3c47"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### `ingest_merge.py`\n",
    "Combina una o varias entradas en un único DataFrame, deduplica el contenido por `id`, aplica normalización ligera y guarda `merged.parquet` + meta. Se le da como input el archivo `.db` o los `.csv` deseados, produciendo en la salida un DF en formato `.parquet`."
   ],
   "id": "25430a09c3406432"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T20:29:17.041479Z",
     "start_time": "2025-08-27T20:29:17.010221Z"
    }
   },
   "cell_type": "code",
   "source": "print(os.getcwd())",
   "id": "159d1393077e3707",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\diego\\Desktop\\Mis Documentos\\0. TFG\\BERTolto\\notebooks_BERTolto\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T20:29:17.125042Z",
     "start_time": "2025-08-27T20:29:17.091058Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define la ruta de tu base de datos tal como la usas\n",
    "db_gh = \"../data/gh_comments/train-fine_tuning/gh_dataset_lastyear.db\"\n",
    "\n",
    "try:\n",
    "    con = sqlite3.connect(db_gh)\n",
    "    # Intenta leer algunas filas de la tabla 'gh_dataset'\n",
    "    test_df = pd.read_sql(\"SELECT * FROM gh_dataset LIMIT 5\", con)\n",
    "    con.close()\n",
    "\n",
    "    print(f\"\\nVerificación de la base de datos '{db_gh}':\")\n",
    "    print(f\"Número de filas recuperadas de 'gh_dataset': {len(test_df)}\")\n",
    "    if not test_df.empty:\n",
    "        print(\"Primeras filas de 'gh_dataset' (limitadas a 5):\")\n",
    "        print(test_df.head())\n",
    "    else:\n",
    "        print(\"¡Advertencia! La tabla 'gh_dataset' está vacía o no se pudieron recuperar datos.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Ocurrió un error al intentar verificar la base de datos: {e}\")\n",
    "    if 'gh_dataset_lastyear.db' not in str(e) and 'no such table' in str(e):\n",
    "        print(\"Asegúrate de que la base de datos y la tabla ('gh_dataset') existen y el nombre es correcto.\")"
   ],
   "id": "2e6a26f024264fe1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verificación de la base de datos '../data/gh_comments/train-fine_tuning/gh_dataset_lastyear.db':\n",
      "Número de filas recuperadas de 'gh_dataset': 5\n",
      "Primeras filas de 'gh_dataset' (limitadas a 5):\n",
      "                                 repo  is_pr  issue_number   comment_type  \\\n",
      "0  electron-userland/electron-builder      0           690  issue_comment   \n",
      "1  electron-userland/electron-builder      0          2674  issue_comment   \n",
      "2  electron-userland/electron-builder      0          3009  issue_comment   \n",
      "3  electron-userland/electron-builder      0          3009  issue_comment   \n",
      "4  electron-userland/electron-builder      0          3124  issue_comment   \n",
      "\n",
      "                                comment_id         comment_created_at  \\\n",
      "0  github_issuecomment_IC_kwDOAiVL48626ksv  2025-07-14 10:14:03+00:00   \n",
      "1  github_issuecomment_IC_kwDOAiVL4866KmSG  2025-07-26 20:27:33+00:00   \n",
      "2  github_issuecomment_IC_kwDOAiVL486w_HsW  2025-06-13 07:10:06+00:00   \n",
      "3  github_issuecomment_IC_kwDOAiVL486x74HL  2025-06-18 18:12:08+00:00   \n",
      "4  github_issuecomment_IC_kwDOAiVL486yE1aY  2025-06-19 10:41:44+00:00   \n",
      "\n",
      "  comment_author  \\\n",
      "0      Wiktor102   \n",
      "1   siikakamania   \n",
      "2         theIYD   \n",
      "3        prayash   \n",
      "4     minhtan143   \n",
      "\n",
      "                                                                                                                                                                                                      text  \\\n",
      "0  Can confirm that specifying the appUrl option for electron-release-server solves the problem. You just need to specify it in the correct place and set the proper port (ex., I use Docker, so I had ...   \n",
      "1  Ok found problem by elimination. I have folder of videos in the app which is 4.54Gb which I need to package with the app. If I run the above config but remove the folder it works. Is there a limit...   \n",
      "2                                                                                 Anyone who was able to crack auto-update with this? I need a setup for auto-update with NSIS installer only for Windows.   \n",
      "3                                                                                                                                                                                                 nice mna   \n",
      "4                                                                                                                                                                              This is a necessary feature   \n",
      "\n",
      "                                                                                 comment_url  \\\n",
      "0   https://github.com/electron-userland/electron-builder/issues/690#issuecomment-3068807983   \n",
      "1  https://github.com/electron-userland/electron-builder/issues/2674#issuecomment-3123340422   \n",
      "2  https://github.com/electron-userland/electron-builder/issues/3009#issuecomment-2969336598   \n",
      "3  https://github.com/electron-userland/electron-builder/issues/3009#issuecomment-2985263563   \n",
      "4  https://github.com/electron-userland/electron-builder/issues/3124#issuecomment-2987611800   \n",
      "\n",
      "                                      context_id  \\\n",
      "0   electron-userland/electron-builder#issue:690   \n",
      "1  electron-userland/electron-builder#issue:2674   \n",
      "2  electron-userland/electron-builder#issue:3009   \n",
      "3  electron-userland/electron-builder#issue:3009   \n",
      "4  electron-userland/electron-builder#issue:3124   \n",
      "\n",
      "                                                           container_title  \\\n",
      "0  Error: Filename can either be an absolute HTTP[s] URL, *or* a file name   \n",
      "1                                          makensis.exe exited with code 1   \n",
      "2                                     How to use electron-forge-maker-nsis   \n",
      "3                                     How to use electron-forge-maker-nsis   \n",
      "4                             How to add a custom page/field to NSIS setup   \n",
      "\n",
      "  container_state  \\\n",
      "0          CLOSED   \n",
      "1          CLOSED   \n",
      "2          CLOSED   \n",
      "3          CLOSED   \n",
      "4          CLOSED   \n",
      "\n",
      "                                                       container_url  \\\n",
      "0   https://github.com/electron-userland/electron-builder/issues/690   \n",
      "1  https://github.com/electron-userland/electron-builder/issues/2674   \n",
      "2  https://github.com/electron-userland/electron-builder/issues/3009   \n",
      "3  https://github.com/electron-userland/electron-builder/issues/3009   \n",
      "4  https://github.com/electron-userland/electron-builder/issues/3124   \n",
      "\n",
      "   container_created_at  container_updated_at  \\\n",
      "0  2016-08-22T17:50:08Z  2025-07-14T10:14:03Z   \n",
      "1  2018-03-08T12:31:10Z  2025-07-26T20:27:33Z   \n",
      "2  2018-06-12T16:48:57Z  2025-06-18T18:12:08Z   \n",
      "3  2018-06-12T16:48:57Z  2025-06-18T18:12:08Z   \n",
      "4  2018-07-17T20:19:17Z  2025-06-19T10:41:44Z   \n",
      "\n",
      "                                   container_labels  \n",
      "0  help wanted;investigate;backlog;Squirrel.Windows  \n",
      "1                                              None  \n",
      "2                                           backlog  \n",
      "3                                           backlog  \n",
      "4                                           backlog  \n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T20:29:38.761117Z",
     "start_time": "2025-08-27T20:29:17.244108Z"
    }
   },
   "cell_type": "code",
   "source": [
    "base = \"../src/data_prep\"\n",
    "\n",
    "# 1. Ingesta + merge (de CSVs o db en SQLite)\n",
    "# Como ya tengo el dataset directamente almacenado en un .db\n",
    "subprocess.run([sys.executable, f\"{base}/ingest_merge.py\",\n",
    "                \"--sqlite-db\", \"../data/gh_comments/train-fine_tuning/gh_dataset_lastyear.db\",\n",
    "                \"--table\", \"gh_dataset\",\n",
    "                \"--out-parquet\", \"../src/artifacts/prep/merged.parquet\",\n",
    "                \"--out-meta\", \"../src/artifacts/prep/merged_meta.json\"\n",
    "                ], check=True)\n"
   ],
   "id": "2087c680aa62567a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['C:\\\\Users\\\\diego\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\python.exe', '../src/data_prep/ingest_merge.py', '--sqlite-db', '../data/gh_comments/train-fine_tuning/gh_dataset_lastyear.db', '--table', 'gh_dataset', '--out-parquet', '../src/artifacts/prep/merged.parquet', '--out-meta', '../src/artifacts/prep/merged_meta.json'], returncode=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### `quick_report.py`\n",
    "Muestra por pantalla un resumen del Parquet aportado en el input: nº de filas, distribución de `label`, distribución de `source`, rango de `created_at`.\n",
    "Es útil para la verificación visual de que la ingesta es correcta antes del split/tokenización e ideal para detectar desbalanceos fuertes o rangos temporales inesperados antes de la fase de entrenamiento."
   ],
   "id": "4f941dffedc431b4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T20:29:41.975183Z",
     "start_time": "2025-08-27T20:29:38.781120Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Informe rápido\n",
    "subprocess.run([sys.executable, f\"{base}/quick_report.py\",\n",
    "                \"--in-parquet\", \"../src/artifacts/prep/merged.parquet\"\n",
    "                ], check=True)"
   ],
   "id": "c054999514390b34",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['C:\\\\Users\\\\diego\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\python.exe', '../src/data_prep/quick_report.py', '--in-parquet', '../src/artifacts/prep/merged.parquet'], returncode=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### `split_thread_temporal.py`\n",
    "Divide el DF `merged.parquet` en train/dev/test respetando hilos (`context_id`) y orden temporal para evitar fuga de información entre splits manteniendo el orden cronológico dentro de cada uno.\n",
    "Los `ratios` son las proporciones con las que se dividirá el dataset:\n",
    "- train: 70%\n",
    "- dev: 15%\n",
    "- test: 15%"
   ],
   "id": "cf0c11c11a32612a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T20:29:45.778659Z",
     "start_time": "2025-08-27T20:29:41.984921Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Split temporal + thread-aware\n",
    "subprocess.run([sys.executable, f\"{base}/split_thread_temporal.py\",\n",
    "                \"--in-parquet\", \"../src/artifacts/prep/merged.parquet\",\n",
    "                \"--out-train\", \"../src/artifacts/prep/split-train.parquet\",\n",
    "                \"--out-dev\", \"../src/artifacts/prep/split-dev.parquet\",\n",
    "                \"--out-test\", \"../src/artifacts/prep/split-test.parquet\",\n",
    "                \"--ratios\", \"0.7\", \"0.15\", \"0.15\",\n",
    "                \"--out-meta\", \"../src/artifacts/prep/split-meta.json\"\n",
    "                ], check=True)"
   ],
   "id": "4c116173457956d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['C:\\\\Users\\\\diego\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\python.exe', '../src/data_prep/split_thread_temporal.py', '--in-parquet', '../src/artifacts/prep/merged.parquet', '--out-train', '../src/artifacts/prep/split-train.parquet', '--out-dev', '../src/artifacts/prep/split-dev.parquet', '--out-test', '../src/artifacts/prep/split-test.parquet', '--ratios', '0.7', '0.15', '0.15', '--out-meta', '../src/artifacts/prep/split-meta.json'], returncode=0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### `tokenize_hf.py`\n",
    "Carga los Parquet de train/dev/test, antepone un prefijo de dominio (`<GITHUB>`, `<REDDIT>`, en este caso no porque todavía estoy solo con los comentarios de GH), tokeniza los textos con el tokenizador de DistilRoBERTa/RoBERTa y guarda:\n",
    "- `dataset/` (formato `save_to_disk` de HF Datasets)\n",
    "- `tokenizer/` (vocabulario + config)\n",
    "- `preprocess_meta.json` (tamaños, `max_len`, pesos de cada clase)\n",
    "\n",
    "#### Parámetros\n",
    "- `--base-model`: modelo que se utilizará, en este caso distilroberta\n",
    "- `--max-len 384`: longitud de secuencia; bajar acelera, subir captura más contexto.\n",
    "- `--use-domain-prefix`: etiqueta de la fuente (GITHUB/REDDIT)\n",
    "- `--sliding-window --slide-stride 128`: para textos muy largos (menos truncado)"
   ],
   "id": "4e94cb88fd74d42c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Para decidir el `--max-len` y la posibilidad de utilizar `--sliding-window`, se puede calcular la distribución de longitudes en tokens y elegir el valor del argumento para cubrir el *p90-p95* y obtener un mejor resultado.",
   "id": "d17eb874affc5ab"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T20:31:47.085495Z",
     "start_time": "2025-08-27T20:29:45.799094Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tok = AutoTokenizer.from_pretrained(\"distilroberta-base\", use_fast=True)\n",
    "df = pd.read_parquet(\"../src/artifacts/prep/merged.parquet\") # o split_train.parquet\n",
    "lens = df[\"text\"].astype(str).map(lambda s: len(tok(s, truncation=False)[\"input_ids\"]))\n",
    "\n",
    "print(pd.Series(lens).describe(percentiles=[.5, .9, .95, .99]))"
   ],
   "id": "ae2900843d1dbb0d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (523 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count   108000.00\n",
      "mean       339.97\n",
      "std       1068.08\n",
      "min          3.00\n",
      "50%         89.00\n",
      "90%        666.00\n",
      "95%       1380.05\n",
      "99%       4944.00\n",
      "max      54918.00\n",
      "Name: text, dtype: float64\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T20:31:50.046873Z",
     "start_time": "2025-08-27T20:31:47.253059Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# tokenización HF (DistilRoBERTa/RoBERTa)\n",
    "subprocess.run([sys.executable, f\"{base}/tokenize_hf.py\",\n",
    "                \"--train-parquet\", \"../src/artifacts/prep/split-train.parquet\",\n",
    "                \"--dev-parquet\", \"../src/artifacts/prep/split-dev.parquet\",\n",
    "                \"--test-parquet\", \"../src/artifacts/prep/split-test.parquet\",\n",
    "                \"--out-dir\", \"../src/artifacts/hf_distilroberta\",\n",
    "                \"--base-model\", \"distilroberta-base\",\n",
    "                \"--max-len\", \"384\"#,\n",
    "                #\"--use-domain-prefix\", # Comentado por ahora que solo hay GH\n",
    "                ], check=True)"
   ],
   "id": "b048947f75cfae36",
   "outputs": [
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['C:\\\\Users\\\\diego\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\python.exe', '../src/data_prep/tokenize_hf.py', '--train-parquet', '../src/artifacts/prep/split-train.parquet', '--dev-parquet', '../src/artifacts/prep/split-dev.parquet', '--test-parquet', '../src/artifacts/prep/split-test.parquet', '--out-dir', '../src/artifacts/hf_distilroberta', '--base-model', 'distilroberta-base', '--max-len', '384']' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mCalledProcessError\u001B[0m                        Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[17], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# tokenización HF (DistilRoBERTa/RoBERTa)\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[43msubprocess\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43msys\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecutable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mbase\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m/tokenize_hf.py\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m--train-parquet\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m../src/artifacts/prep/split-train.parquet\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m--dev-parquet\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m../src/artifacts/prep/split-dev.parquet\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m--test-parquet\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m../src/artifacts/prep/split-test.parquet\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m--out-dir\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m../src/artifacts/hf_distilroberta\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m--base-model\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdistilroberta-base\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      8\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m--max-len\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m384\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;66;43;03m#,\u001B[39;49;00m\n\u001B[0;32m      9\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;66;43;03m#\"--use-domain-prefix\", # Comentado por ahora que solo hay GH\u001B[39;49;00m\n\u001B[0;32m     10\u001B[0m \u001B[43m                \u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcheck\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\subprocess.py:526\u001B[0m, in \u001B[0;36mrun\u001B[1;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001B[0m\n\u001B[0;32m    524\u001B[0m     retcode \u001B[38;5;241m=\u001B[39m process\u001B[38;5;241m.\u001B[39mpoll()\n\u001B[0;32m    525\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m check \u001B[38;5;129;01mand\u001B[39;00m retcode:\n\u001B[1;32m--> 526\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m CalledProcessError(retcode, process\u001B[38;5;241m.\u001B[39margs,\n\u001B[0;32m    527\u001B[0m                                  output\u001B[38;5;241m=\u001B[39mstdout, stderr\u001B[38;5;241m=\u001B[39mstderr)\n\u001B[0;32m    528\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m CompletedProcess(process\u001B[38;5;241m.\u001B[39margs, retcode, stdout, stderr)\n",
      "\u001B[1;31mCalledProcessError\u001B[0m: Command '['C:\\\\Users\\\\diego\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\python.exe', '../src/data_prep/tokenize_hf.py', '--train-parquet', '../src/artifacts/prep/split-train.parquet', '--dev-parquet', '../src/artifacts/prep/split-dev.parquet', '--test-parquet', '../src/artifacts/prep/split-test.parquet', '--out-dir', '../src/artifacts/hf_distilroberta', '--base-model', 'distilroberta-base', '--max-len', '384']' returned non-zero exit status 1."
     ]
    }
   ],
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
