{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50b7964ea9a2b2da",
   "metadata": {},
   "source": [
    "R# Preprocesamiento de los comentarios para la primera fase de fine-tuning y entrenamiento del modelo\n",
    "En esta primera fase, una vez extraídos los bodies de todos los issues/PRs de los repositorios listados junto con 2 comentarios de 250 del total de issues de cada repositorio, se prepararán todos los datos para ser utilizados en el entrenamiento y ajuste fino del modelo que se utilizará.\n",
    "\n",
    "Se seguirán prácticamente los mismos pasos vistos en *c_preparing_data_for_statistics_and_ML* pero con varias diferencias claves que existen entre los modelos BERT que se utilizarán ahora y los modelos de clasificación presentados con anterioridad (notebooks de GVTIA).\n",
    "\n",
    "Para Transformers funciona mejor un preprocesado mínimo y dejar la segmentación al propio tokenizador del modelo, a continuación se muestra que procedimientos similares a los anteriores se mantendrán y cuáles se evitarán:\n",
    "\n",
    "## Se mantendrá:\n",
    "- Normalización de espacios/saltos de línea\n",
    "- Eliminación de caracteres de control raros o poco usuales\n",
    "- Se conservará el uso de mayúsculas y minúsculas, signos, números, URLs, nombres propios de vulnerabilidades o bugs (CVE-XXXX-YYYY), rutas (/etc/...), código entre backticks (`return salida`), nombres de APIs.\n",
    "- Se definirá una longitud máxima de tokens por comentario o el uso de *sliding window* si el texto es muy largo.\n",
    "\n",
    "## Se omitirá:\n",
    "- Pasar todo el texto a minúsculas, los modelos RoBERTa/DistilRoBERTa que se utilizarán utilizan mayúsculas y minúsculas.\n",
    "- Eliminar la puntuación y stopwords.\n",
    "- Stemming / lematización.\n",
    "- Normalizaciones agresivas de URLs/código -> se pierde señal técnica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decd1abca07ad89a",
   "metadata": {},
   "source": [
    "Una vez explicado esto, se comenzará con el preprocesado de todos los comentarios extraídos de GitHub, comenzando como se ha visto ya en diversas ocasiones, con cargar el documento (.csv) en un dataframe de pandas para su uso y manipulación.\n",
    "\n",
    "En este caso, se cuenta con 2 documentos:\n",
    "- **gh_bodys_lastyear.csv**. Archivo que contiene los bodies (comentario principal) de todos los Issues/PRs en el último año de los repositorios listados para la extracción de comentarios.\n",
    "- **gh_comments_lastyear.csv**. Archivo que contiene los 2 primeros comentarios de cada Issue/PR de 250 Issues/PRs por repositorio (500 comentarios por repo), en gran parte de los casos serán las respuestas aportadas por usuarios al body del documento anterior.\n",
    "\n",
    "En este caso, como se cuenta con 2 documentos lo que se hará es crear 2 dataframes, uno con cada documento, para a continuación unirlos con la función `concat()` de pandas y ordenarlos según el id del Issue/PR para la clara visualización y mantener una estructura coherente entre cuerpo principal y comentarios asociados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ed28d7ff902c74f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T11:02:32.252105Z",
     "start_time": "2025-08-28T11:02:25.714034Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "# Cargamos el archivo settings_bert.py que contiene todos los imports necesarios para la correcta ejecución del notebook y no tener que importarlos en cada celda de código\n",
    "%run -i ../settings_bert.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b67a9537d80fe3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T11:02:34.363558Z",
     "start_time": "2025-08-28T11:02:32.501180Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ruta de los archivos\n",
    "path_gh_bodys = \"../data/gh_comments/training/gh_bodys_lastyear.csv\"\n",
    "path_gh_comments = \"../data/gh_comments/training/gh_comments_lastyear.csv\"\n",
    "\n",
    "# Carga de los archivos en DataFrames\n",
    "df_bodys = pd.read_csv(path_gh_bodys)\n",
    "df_comms = pd.read_csv(path_gh_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ecea4e9efb540d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T11:02:34.412285Z",
     "start_time": "2025-08-28T11:02:34.395806Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['repo', 'is_pr', 'issue_number', 'comment_type', 'comment_id',\n",
      "       'comment_created_at', 'comment_author', 'text', 'comment_url',\n",
      "       'context_id', 'container_title', 'container_state', 'container_url',\n",
      "       'container_created_at', 'container_updated_at', 'container_labels'],\n",
      "      dtype='object')\n",
      "Index(['kubernetes/kubernetes', 'False', '133680', 'issue_comment',\n",
      "       'github_issuecomment_IC_kwDOAToIks6_4TOW', '2025-08-25T07:51:17Z',\n",
      "       'k8s-ci-robot',\n",
      "       'This issue is currently awaiting triage.\\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the triage/accepted label and provide further guidance.\\nThe triage/accepted label can be added by org members by writing /triage accepted in a comment.\\n\\nInstructions for interacting with me using PR comments are available here.  If you have questions or suggestions related to my behavior, please file an issue against the kubernetes-sigs/prow repository.',\n",
      "       'https://github.com/kubernetes/kubernetes/issues/133680#issuecomment-3219207062',\n",
      "       'kubernetes/kubernetes#issue:133680',\n",
      "       'Add new imagePullPolicy mode IfNewerNotPresent, that pulls image if image ID (ref/digest) on remote is not matching.',\n",
      "       'OPEN', 'https://github.com/kubernetes/kubernetes/issues/133680',\n",
      "       '2025-08-25T07:51:08Z', '2025-08-26T08:18:05Z',\n",
      "       'sig/node;kind/feature;needs-triage'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_bodys.columns)\n",
    "print(df_comms.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928929f1de768f4e",
   "metadata": {},
   "source": [
    "Se ha cometido un error en la escritura de la cabecera de los comentarios por escribir siempre en el mismo documento y borrar su contenido en vez de eliminar el documento antes de comenzar con una nueva extracción. Vamos a tratar de repararlo sin tener que volver a realizar todo el proceso de extracción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3034471a4637f6e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T11:02:36.888819Z",
     "start_time": "2025-08-28T11:02:34.439498Z"
    }
   },
   "outputs": [],
   "source": [
    "path_gh_bodys = Path(path_gh_bodys)\n",
    "path_gh_comments = Path(path_gh_comments)\n",
    "\n",
    "EXPECTED_COLS = [\n",
    "    'repo','is_pr','issue_number','comment_type','comment_id','comment_created_at','comment_author',\n",
    "    'text','comment_url','context_id','container_title','container_state','container_url',\n",
    "    'container_created_at','container_updated_at','container_labels'\n",
    "]\n",
    "\n",
    "def read_with_header_fix(p: Path) -> pd.DataFrame:\n",
    "    # Se lee 1 fila para inspeccionar columnas\n",
    "    probe = pd.read_csv(p, nrows=1)\n",
    "    if list(probe.columns) == EXPECTED_COLS:\n",
    "        return pd.read_csv(p)\n",
    "    # Si no coincide, reinterpretamos: no hay cabecera -> header=None + names=EXPECTED_COLS\n",
    "    return pd.read_csv(p, header=None, names=EXPECTED_COLS)\n",
    "\n",
    "df_bodys = read_with_header_fix(path_gh_bodys)\n",
    "df_comms = read_with_header_fix(path_gh_comments)\n",
    "\n",
    "# Se unen ambos DataFrames\n",
    "df_gh = pd.concat([df_bodys, df_comms], ignore_index=True)\n",
    "\n",
    "# Tipos y ordenación\n",
    "df_gh['comment_created_at'] = pd.to_datetime(df_gh['comment_created_at'], errors='coerce', utc=True)\n",
    "df_gh.loc[df_gh['comment_created_at'].isna(), 'comment_created_at'] = pd.to_datetime(df_gh['container_created_at'], errors='coerce', utc=True)\n",
    "\n",
    "order_map = {'issue_body':0, 'pr_body':0} # Bodies primero -> coherencia\n",
    "df_gh['order'] = df_gh['comment_type'].map(order_map).fillna(1).astype(int)\n",
    "\n",
    "df_gh = df_gh.sort_values(by=['repo','issue_number','order','comment_created_at','comment_id'], kind='mergesort').drop(columns=['order'])\n",
    "\n",
    "# Normalizar booleano -> OPCIONAL\n",
    "df_gh['is_pr'] = df_gh['is_pr'].astype(str).str.lower().map({'true':True, 'false':False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7895ae432b8ec3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T11:02:36.929763Z",
     "start_time": "2025-08-28T11:02:36.916014Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['repo', 'is_pr', 'issue_number', 'comment_type', 'comment_id',\n",
      "       'comment_created_at', 'comment_author', 'text', 'comment_url',\n",
      "       'context_id', 'container_title', 'container_state', 'container_url',\n",
      "       'container_created_at', 'container_updated_at', 'container_labels'],\n",
      "      dtype='object')\n",
      "Index(['repo', 'is_pr', 'issue_number', 'comment_type', 'comment_id',\n",
      "       'comment_created_at', 'comment_author', 'text', 'comment_url',\n",
      "       'context_id', 'container_title', 'container_state', 'container_url',\n",
      "       'container_created_at', 'container_updated_at', 'container_labels'],\n",
      "      dtype='object')\n",
      "108078\n"
     ]
    }
   ],
   "source": [
    "# Muestra para comprobar que se ha ejecutado correctamente\n",
    "print(df_bodys.columns)\n",
    "print(df_comms.columns)\n",
    "\n",
    "df_gh.head(10).T\n",
    "print(len(df_gh))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65043b29d28ed666",
   "metadata": {},
   "source": [
    "Ahora sí están todos los comentarios bien ordenados. Antes de comenzar con el preprocesado vamos a guardar el dataframe en una base de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b425f5858db09200",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T11:02:36.969658Z",
     "start_time": "2025-08-28T11:02:36.956500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108078\n"
     ]
    }
   ],
   "source": [
    "# Compruebo que todos los comentarios se han almacenado correctamente en el DF\n",
    "print(len(df_gh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de15d1140cc5ed4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T11:02:42.308133Z",
     "start_time": "2025-08-28T11:02:37.002367Z"
    }
   },
   "outputs": [],
   "source": [
    "db_gh = \"../data/gh_comments/train-fine_tuning/gh_dataset_lastyear.db\"\n",
    "con = sqlite3.connect(db_gh)\n",
    "df_gh.to_sql('gh_dataset', con, if_exists='replace', index=False)\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b3a603ab9a704c",
   "metadata": {},
   "source": [
    "Ahora sí se procederá al procesamiento del dataset para dejarlo preparado para el modelo BERT que se utilizará, RoBERTa o DistilRoBERTa. Este proceso se va a definir en una serie de scripts .py, cada uno con el objetivo de realizar una tarea para su reutilización en otros puntos del proyecto (cuando se haga el de reddit, u otros comentarios de github) de forma que estos sean agnósticos al sistema del que se extraen los comentarios que serán utilizados por el modelo.\n",
    "\n",
    "Del mismo modo, tras el procesamiento de los datos, el resultado del procesado será almacenado en archivos `.parquet` por su ligereza y agilidad a la hora de ser manipulados y consumidos por modelos BERT. Las principales ventajas de este formato son:\n",
    "- Más pequeño: compresión por columna, pesa de 2 a 5 veces menos que un `.csv`\n",
    "- Más rápido: lee solo las columnas que se necesitan (\"column pruning\") y aplica vectorización.\n",
    "- Conserva tipos: fechas, booleanos, enteros \"nullable\", etc. (`.csv`los pierde)\n",
    "- Esquema: guarda el _schema_ dentro del archivo -> menos sorpresas al cargarlo\n",
    "\n",
    "Por estas características el formato es el preferido para pipelines de datos/ML. En este caso la estructura que se utilizará será:\n",
    "- `merged.parquet` = será el dataset completo tras ingesta y normalización ligera.\n",
    "- `split_train.parquet`, `split_dev.parquet`, `split_test.parquet` = particiones del dataset ya divididas, listas para su tokenización.\n",
    "\n",
    "#### Ventajas frente a CSV\n",
    "- Evita problemas de comas y saltos de línea\n",
    "- Mantiene las fechas (`created_at`), booleanos (`is_pr`) y enteros sin perder el tipo.\n",
    "- Carga solo las columnas necesarias -> menor uso de RAM y tiempo de ejecución."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3d14442f2d3c47",
   "metadata": {},
   "source": [
    "## Ejecución del preprocesado de datos\n",
    "Se han definido varios scripts, cada uno con una función en el preprocesado de datos para modelos BERT, a  los cuáles se les llamará desde este notebook con los argumentos correspondientes para realizar este proceso.\n",
    "\n",
    "Las funciones reutilizables para el pipeline están definidas en `prep_utils.py`. Normaliza texto \"ligero\", mapea columnas heterogéneas (GH/Reddit) al esquema core (id, text, label, source, created_at, context_id), lee CSV/SQLite y guarda Parquet.\n",
    "El objetivo principal de estas funciones es su utilización cuando se desea agnosticismo de fuente (Github/Reddit) y un preprocesado mínimo ideal para Transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25430a09c3406432",
   "metadata": {},
   "source": [
    "### `ingest_merge.py`\n",
    "Combina una o varias entradas en un único DataFrame, deduplica el contenido por `id`, aplica normalización ligera y guarda `merged.parquet` + meta. Se le da como input el archivo `.db` o los `.csv` deseados, produciendo en la salida un DF en formato `.parquet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2087c680aa62567a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T11:02:50.399660Z",
     "start_time": "2025-08-28T11:02:42.336276Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['C:\\\\Users\\\\diego\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\python.exe', '../src/data_prep/ingest_merge.py', '--sqlite-db', '../data/gh_comments/train-fine_tuning/gh_dataset_lastyear.db', '--table', 'gh_dataset', '--out-parquet', '../src/artifacts/prep/merged.parquet', '--out-meta', '../src/artifacts/prep/merged_meta.json'], returncode=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base = \"../src/data_prep\"\n",
    "\n",
    "# 1. Ingesta + merge (de CSVs o db en SQLite)\n",
    "# Como ya tengo el dataset directamente almacenado en un .db\n",
    "subprocess.run([sys.executable, f\"{base}/ingest_merge.py\",\n",
    "                \"--sqlite-db\", \"../data/gh_comments/train-fine_tuning/gh_dataset_lastyear.db\",\n",
    "                \"--table\", \"gh_dataset\",\n",
    "                \"--out-parquet\", \"../src/artifacts/prep/merged.parquet\",\n",
    "                \"--out-meta\", \"../src/artifacts/prep/merged_meta.json\"\n",
    "                ], check=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f941dffedc431b4",
   "metadata": {},
   "source": [
    "### `quick_report.py`\n",
    "Muestra por pantalla un resumen del Parquet aportado en el input: nº de filas, distribución de `label`, distribución de `source`, rango de `created_at`.\n",
    "Es útil para la verificación visual de que la ingesta es correcta antes del split/tokenización e ideal para detectar desbalanceos fuertes o rangos temporales inesperados antes de la fase de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c054999514390b34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T11:02:51.451607Z",
     "start_time": "2025-08-28T11:02:50.409193Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['C:\\\\Users\\\\diego\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\python.exe', '../src/data_prep/quick_report.py', '--in-parquet', '../src/artifacts/prep/merged.parquet'], returncode=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Informe rápido\n",
    "subprocess.run([sys.executable, f\"{base}/quick_report.py\",\n",
    "                \"--in-parquet\", \"../src/artifacts/prep/merged.parquet\"\n",
    "                ], check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0c11c11a32612a",
   "metadata": {},
   "source": [
    "### `split_thread_temporal.py`\n",
    "Divide el DF `merged.parquet` en train/dev/test respetando hilos (`context_id`) y orden temporal para evitar fuga de información entre splits manteniendo el orden cronológico dentro de cada uno.\n",
    "Los `ratios` son las proporciones con las que se dividirá el dataset:\n",
    "- train: 70%\n",
    "- dev: 15%\n",
    "- test: 15%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c116173457956d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T11:02:53.261296Z",
     "start_time": "2025-08-28T11:02:51.484079Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['C:\\\\Users\\\\diego\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\python.exe', '../src/data_prep/split_thread_temporal.py', '--in-parquet', '../src/artifacts/prep/merged.parquet', '--out-train', '../src/artifacts/prep/split_train.parquet', '--out-dev', '../src/artifacts/prep/split_dev.parquet', '--out-test', '../src/artifacts/prep/split_test.parquet', '--ratios', '0.7', '0.15', '0.15', '--out-meta', '../src/artifacts/prep/split_meta.json'], returncode=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split temporal + thread-aware\n",
    "subprocess.run([sys.executable, f\"{base}/split_thread_temporal.py\",\n",
    "                \"--in-parquet\", \"../src/artifacts/prep/merged.parquet\",\n",
    "                \"--out-train\", \"../src/artifacts/prep/split_train.parquet\",\n",
    "                \"--out-dev\", \"../src/artifacts/prep/split_dev.parquet\",\n",
    "                \"--out-test\", \"../src/artifacts/prep/split_test.parquet\",\n",
    "                \"--ratios\", \"0.7\", \"0.15\", \"0.15\",\n",
    "                \"--out-meta\", \"../src/artifacts/prep/split_meta.json\"\n",
    "                ], check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e94cb88fd74d42c",
   "metadata": {},
   "source": [
    "### `tokenize_hf.py`\n",
    "Carga los Parquet de train/dev/test, antepone un prefijo de dominio (`<GITHUB>`, `<REDDIT>`, en este caso no porque todavía estoy solo con los comentarios de GH), tokeniza los textos con el tokenizador de DistilRoBERTa/RoBERTa y guarda:\n",
    "- `dataset/` (formato `save_to_disk` de HF Datasets)\n",
    "- `tokenizer/` (vocabulario + config)\n",
    "- `preprocess_meta.json` (tamaños, `max_len`, pesos de cada clase)\n",
    "\n",
    "#### Parámetros\n",
    "- `--base-model`: modelo que se utilizará, en este caso distilroberta\n",
    "- `--max-len 384`: longitud de secuencia; bajar acelera, subir captura más contexto.\n",
    "- `--use-domain-prefix`: etiqueta de la fuente (GITHUB/REDDIT)\n",
    "- `--sliding-window --slide-stride 128`: para textos muy largos (menos truncado)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17eb874affc5ab",
   "metadata": {},
   "source": [
    "Para decidir el `--max-len` y la posibilidad de utilizar `--sliding-window`, se puede calcular la distribución de longitudes en tokens y elegir el valor del argumento para cubrir el *p90-p95* y obtener un mejor resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae2900843d1dbb0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T11:03:59.656217Z",
     "start_time": "2025-08-28T11:02:53.277800Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (523 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count   108000.00\n",
      "mean       339.97\n",
      "std       1068.08\n",
      "min          3.00\n",
      "50%         89.00\n",
      "90%        666.00\n",
      "95%       1380.05\n",
      "99%       4944.00\n",
      "max      54918.00\n",
      "Name: text, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "tok = AutoTokenizer.from_pretrained(\"distilroberta-base\", use_fast=True)\n",
    "df = pd.read_parquet(\"../src/artifacts/prep/merged.parquet\") # o split_train.parquet\n",
    "lens = df[\"text\"].astype(str).map(lambda s: len(tok(s, truncation=False)[\"input_ids\"]))\n",
    "\n",
    "print(pd.Series(lens).describe(percentiles=[.5, .9, .95, .99]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d790f7a5690dee84",
   "metadata": {},
   "source": [
    "### Argumentos del tokenizador\n",
    "- `--max-len 384`. Como el modelo solo puede leer hasta N tokens al mismo tiempo, se limita a N tokens (384) en la tokenización de cada texto. Si este tiene un nº superior se trunca o se divide en trozos (sliding-window). Elijo 384 tokens máx. por texto por su equilibrio en velocidad y uso de VRAM (no debería de tener problema con mi PC).\n",
    "- `--sliding-window`. Si un texto no cabe en `--max-len`, en lugar de descartar el sobrante se divide en ventanas solapadas, dando como resultado varias filas por cada texto largo.\n",
    "- `--slide-stride 128`. Es el número de tokens que se solapan entre ventanas para no cortar frases/palabras a la mitad cuando se divide un texto entre ventanas. En este caso si el texto se divide en 2 ventanas la distribución sería:\n",
    "    - Ventana 1: tokens 1-384\n",
    "    - Ventana 2: tokens (384-128) 256-639\n",
    "- `--max-windows-per-doc 6`. Limita a un máximo de 6 ventanas por texto, el resto se recorta. Es mejor que descartar el texto completo, y si supera el máx. de ventanas es muy probable que sea debido a mensajes de logs o diffs y no aporte nada al modelo, solo ruido.\n",
    "- `--filter-max-input-tokens`. Como se ve en la celda anterior (porcentajes) hay textos enormes y con este argumento se descartan sus colas limitando a N tokens antes de dividir en ventanas.\n",
    "- `--num-proc 12`. Usa 12 hilos de ejecución para map paralelo (Mi PC tiene 8C/16T) -> Implementar en el tokenizador\n",
    "- `--batch-chunk-size 512`. Indica el tamaño de trozos (grandes) en cada llamada al tokenizador. -> Implementar en el tokenizador\n",
    "\n",
    "#### Recomendaciones\n",
    "- Si ves que sube mucho el nº de ejemplos → baja --max-windows-per-doc a 4.\n",
    "- Si va justo de RAM → --num-proc 8 y/o --batch-chunk-size 256."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b048947f75cfae36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T11:05:52.810511Z",
     "start_time": "2025-08-28T11:03:59.698438Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[filtro] train -> 263 descartados por > 8192 tokens\n",
      "[filtro] validation -> 36 descartados por > 8192 tokens\n",
      "[filtro] test -> 37 descartados por > 8192 tokens\n",
      "OK -> ..\\src\\artifacts\\hf_distilroberta\\dataset ..\\src\\artifacts\\hf_distilroberta\\tokenizer\n",
      "\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (848 > 384). Running this sequence through the model will result in indexing errors\n",
      "\n",
      "Map (num_proc=8):   0%|          | 0/73841 [00:00<?, ? examples/s]\n",
      "Map (num_proc=8):   1%|          | 512/73841 [00:08<21:08, 57.82 examples/s]\n",
      "Map (num_proc=8):   1%|▏         | 1024/73841 [00:09<09:04, 133.80 examples/s]\n",
      "Map (num_proc=8):   3%|▎         | 2048/73841 [00:09<03:28, 344.65 examples/s]\n",
      "Map (num_proc=8):   3%|▎         | 2560/73841 [00:09<02:31, 470.08 examples/s]\n",
      "Map (num_proc=8):   6%|▌         | 4096/73841 [00:09<01:08, 1017.63 examples/s]\n",
      "Map (num_proc=8):   6%|▌         | 4608/73841 [00:09<00:59, 1163.15 examples/s]\n",
      "Map (num_proc=8):   8%|▊         | 6144/73841 [00:10<00:34, 1938.70 examples/s]\n",
      "Map (num_proc=8):  10%|▉         | 7168/73841 [00:10<00:26, 2517.27 examples/s]\n",
      "Map (num_proc=8):  11%|█         | 8192/73841 [00:10<00:21, 3097.00 examples/s]\n",
      "Map (num_proc=8):  12%|█▏        | 9216/73841 [00:10<00:17, 3661.95 examples/s]\n",
      "Map (num_proc=8):  15%|█▍        | 10752/73841 [00:10<00:12, 5037.72 examples/s]\n",
      "Map (num_proc=8):  16%|█▌        | 11776/73841 [00:10<00:11, 5336.23 examples/s]\n",
      "Map (num_proc=8):  18%|█▊        | 13312/73841 [00:11<00:09, 6623.95 examples/s]\n",
      "Map (num_proc=8):  20%|██        | 14848/73841 [00:11<00:07, 7643.24 examples/s]\n",
      "Map (num_proc=8):  21%|██▏       | 15872/73841 [00:11<00:08, 7241.28 examples/s]\n",
      "Map (num_proc=8):  24%|██▎       | 17408/73841 [00:11<00:07, 7562.29 examples/s]\n",
      "Map (num_proc=8):  26%|██▋       | 19456/73841 [00:11<00:06, 8493.35 examples/s]\n",
      "Map (num_proc=8):  28%|██▊       | 20480/73841 [00:11<00:06, 8379.05 examples/s]\n",
      "Map (num_proc=8):  29%|██▉       | 21504/73841 [00:12<00:06, 7687.25 examples/s]\n",
      "Map (num_proc=8):  31%|███       | 23040/73841 [00:12<00:05, 8700.72 examples/s]\n",
      "Map (num_proc=8):  33%|███▎      | 24064/73841 [00:12<00:05, 8315.44 examples/s]\n",
      "Map (num_proc=8):  35%|███▌      | 26112/73841 [00:12<00:06, 7758.35 examples/s]\n",
      "Map (num_proc=8):  39%|███▉      | 28672/73841 [00:12<00:04, 10472.51 examples/s]\n",
      "Map (num_proc=8):  41%|████      | 30208/73841 [00:13<00:05, 8316.65 examples/s] \n",
      "Map (num_proc=8):  42%|████▏     | 31232/73841 [00:13<00:05, 8371.20 examples/s]\n",
      "Map (num_proc=8):  45%|████▌     | 33280/73841 [00:13<00:04, 8368.98 examples/s]\n",
      "Map (num_proc=8):  46%|████▋     | 34304/73841 [00:13<00:04, 8286.52 examples/s]\n",
      "Map (num_proc=8):  48%|████▊     | 35328/73841 [00:13<00:04, 8159.25 examples/s]\n",
      "Map (num_proc=8):  50%|████▉     | 36864/73841 [00:13<00:04, 8975.60 examples/s]\n",
      "Map (num_proc=8):  51%|█████▏    | 37888/73841 [00:13<00:04, 8923.94 examples/s]\n",
      "Map (num_proc=8):  53%|█████▎    | 38912/73841 [00:14<00:05, 6695.09 examples/s]\n",
      "Map (num_proc=8):  55%|█████▌    | 40960/73841 [00:14<00:04, 7351.32 examples/s]\n",
      "Map (num_proc=8):  57%|█████▋    | 41984/73841 [00:14<00:04, 6605.56 examples/s]\n",
      "Map (num_proc=8):  59%|█████▉    | 43520/73841 [00:14<00:03, 7993.16 examples/s]\n",
      "Map (num_proc=8):  61%|██████    | 45056/73841 [00:14<00:03, 8659.62 examples/s]\n",
      "Map (num_proc=8):  62%|██████▏   | 46080/73841 [00:15<00:04, 6857.84 examples/s]\n",
      "Map (num_proc=8):  64%|██████▍   | 47616/73841 [00:15<00:03, 8026.45 examples/s]\n",
      "Map (num_proc=8):  66%|██████▌   | 48640/73841 [00:15<00:03, 7260.69 examples/s]\n",
      "Map (num_proc=8):  67%|██████▋   | 49664/73841 [00:15<00:03, 7519.99 examples/s]\n",
      "Map (num_proc=8):  70%|███████   | 51712/73841 [00:15<00:02, 10127.93 examples/s]\n",
      "Map (num_proc=8):  72%|███████▏  | 53248/73841 [00:15<00:02, 7196.24 examples/s] \n",
      "Map (num_proc=8):  73%|███████▎  | 54272/73841 [00:16<00:02, 7631.64 examples/s]\n",
      "Map (num_proc=8):  75%|███████▍  | 55296/73841 [00:16<00:02, 7136.37 examples/s]\n",
      "Map (num_proc=8):  76%|███████▋  | 56334/73841 [00:16<00:02, 7235.24 examples/s]\n",
      "Map (num_proc=8):  79%|███████▉  | 58382/73841 [00:16<00:01, 7893.60 examples/s]\n",
      "Map (num_proc=8):  80%|████████  | 59406/73841 [00:16<00:02, 6963.90 examples/s]\n",
      "Map (num_proc=8):  83%|████████▎ | 61454/73841 [00:17<00:01, 7935.91 examples/s]\n",
      "Map (num_proc=8):  85%|████████▍ | 62478/73841 [00:17<00:01, 7947.42 examples/s]\n",
      "Map (num_proc=8):  86%|████████▌ | 63502/73841 [00:17<00:01, 8234.68 examples/s]\n",
      "Map (num_proc=8):  87%|████████▋ | 64526/73841 [00:17<00:01, 7507.99 examples/s]\n",
      "Map (num_proc=8):  90%|█████████ | 66574/73841 [00:17<00:01, 7175.85 examples/s]\n",
      "Map (num_proc=8):  92%|█████████▏| 68124/73841 [00:17<00:00, 7958.61 examples/s]\n",
      "Map (num_proc=8):  94%|█████████▎| 69162/73841 [00:18<00:00, 6417.96 examples/s]\n",
      "Map (num_proc=8):  96%|█████████▌| 70726/73841 [00:18<00:00, 5140.77 examples/s]\n",
      "Map (num_proc=8):  97%|█████████▋| 71764/73841 [00:18<00:00, 5519.71 examples/s]\n",
      "Map (num_proc=8):  99%|█████████▊| 72803/73841 [00:19<00:00, 3391.97 examples/s]\n",
      "Map (num_proc=8):  99%|█████████▉| 73315/73841 [00:19<00:00, 2716.53 examples/s]\n",
      "Map (num_proc=8): 100%|█████████▉| 73827/73841 [00:20<00:00, 2349.25 examples/s]\n",
      "Map (num_proc=8): 100%|██████████| 73841/73841 [00:21<00:00, 3363.27 examples/s]\n",
      "\n",
      "Map (num_proc=8):   0%|          | 0/16293 [00:00<?, ? examples/s]\n",
      "Map (num_proc=8):   3%|▎         | 512/16293 [00:08<04:24, 59.62 examples/s]\n",
      "Map (num_proc=8):   9%|▉         | 1536/16293 [00:08<01:05, 223.64 examples/s]\n",
      "Map (num_proc=8):  22%|██▏       | 3584/16293 [00:08<00:18, 681.01 examples/s]\n",
      "Map (num_proc=8):  31%|███▏      | 5120/16293 [00:09<00:10, 1115.77 examples/s]\n",
      "Map (num_proc=8):  41%|████      | 6656/16293 [00:09<00:05, 1635.55 examples/s]\n",
      "Map (num_proc=8):  53%|█████▎    | 8704/16293 [00:09<00:03, 2527.24 examples/s]\n",
      "Map (num_proc=8):  63%|██████▎   | 10240/16293 [00:09<00:01, 3280.29 examples/s]\n",
      "Map (num_proc=8):  72%|███████▏  | 11776/16293 [00:09<00:01, 4150.83 examples/s]\n",
      "Map (num_proc=8):  82%|████████▏ | 13289/16293 [00:09<00:00, 5222.91 examples/s]\n",
      "Map (num_proc=8):  91%|█████████ | 14791/16293 [00:09<00:00, 6351.14 examples/s]\n",
      "Map (num_proc=8): 100%|██████████| 16293/16293 [00:10<00:00, 6612.93 examples/s]\n",
      "Map (num_proc=8): 100%|██████████| 16293/16293 [00:11<00:00, 1398.61 examples/s]\n",
      "\n",
      "Map (num_proc=8):   0%|          | 0/17530 [00:00<?, ? examples/s]\n",
      "Map (num_proc=8):   3%|▎         | 512/17530 [00:08<04:46, 59.46 examples/s]\n",
      "Map (num_proc=8):  12%|█▏        | 2048/17530 [00:08<00:50, 306.27 examples/s]\n",
      "Map (num_proc=8):  18%|█▊        | 3072/17530 [00:08<00:27, 527.79 examples/s]\n",
      "Map (num_proc=8):  29%|██▉       | 5120/17530 [00:09<00:10, 1140.03 examples/s]\n",
      "Map (num_proc=8):  38%|███▊      | 6656/17530 [00:09<00:06, 1653.31 examples/s]\n",
      "Map (num_proc=8):  55%|█████▌    | 9728/17530 [00:09<00:02, 3138.27 examples/s]\n",
      "Map (num_proc=8):  64%|██████▍   | 11264/17530 [00:09<00:01, 3571.03 examples/s]\n",
      "Map (num_proc=8):  79%|███████▉  | 13824/17530 [00:09<00:00, 5311.98 examples/s]\n",
      "Map (num_proc=8):  88%|████████▊ | 15421/17530 [00:10<00:00, 5521.58 examples/s]\n",
      "Map (num_proc=8):  98%|█████████▊| 17243/17530 [00:10<00:00, 6833.64 examples/s]\n",
      "Map (num_proc=8): 100%|██████████| 17530/17530 [00:11<00:00, 1497.80 examples/s]\n",
      "\n",
      "Saving the dataset (0/1 shards):   0%|          | 0/114631 [00:00<?, ? examples/s]\n",
      "Saving the dataset (0/1 shards):  91%|█████████ | 104000/114631 [00:00<00:00, 877790.82 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 114631/114631 [00:00<00:00, 877790.82 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 114631/114631 [00:00<00:00, 906325.01 examples/s]\n",
      "\n",
      "Saving the dataset (0/1 shards):   0%|          | 0/23877 [00:00<?, ? examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 23877/23877 [00:00<00:00, 852746.46 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 23877/23877 [00:00<00:00, 852746.46 examples/s]\n",
      "\n",
      "Saving the dataset (0/1 shards):   0%|          | 0/24396 [00:00<?, ? examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 24396/24396 [00:00<00:00, 871282.09 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 24396/24396 [00:00<00:00, 871282.09 examples/s]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = dict(os.environ)\n",
    "\n",
    "# Fuerza UTF-8 en el hijo:\n",
    "env[\"PYTHONIOENCODING\"] = \"utf-8\"\n",
    "env[\"PYTHONUTF8\"] = \"1\"\n",
    "\n",
    "# Paralelización\n",
    "env[\"TOKENIZERS_PARALLELISM\"] = \"false\" #evita sub-proceso\n",
    "env[\"RAYON_NUM_THREADS\"] = \"2\" #2 hilos por proceso\n",
    "\n",
    "# tokenización HF (DistilRoBERTa/RoBERTa)\n",
    "res = subprocess.run([sys.executable, f\"{base}/tokenize_hf.py\",\n",
    "                \"--train-parquet\", \"../src/artifacts/prep/split_train.parquet\",\n",
    "                \"--dev-parquet\", \"../src/artifacts/prep/split_dev.parquet\",\n",
    "                \"--test-parquet\", \"../src/artifacts/prep/split_test.parquet\",\n",
    "                \"--out-dir\", \"../src/artifacts/hf_distilroberta\",\n",
    "                \"--base-model\", \"distilroberta-base\",\n",
    "                \"--max-len\", \"384\",\n",
    "                \"--sliding-window\",\n",
    "                \"--slide-stride\", \"128\",\n",
    "                \"--max-window-per-doc\", \"8\",\n",
    "                \"--filter-max-input-tokens\", \"8192\",\n",
    "                #\"--use-domain-prefix\", # Se activa cuando incluya Reddit\n",
    "                \"--num-proc\", \"8\",\n",
    "                \"--batch-chunk-size\", \"512\"\n",
    "                ], check=True, env=env, capture_output=True, text=True, encoding=\"utf-8\", errors=\"replace\")\n",
    "\n",
    "print(res.stdout)   # logs del script\n",
    "print(res.stderr)   # avisos/tracebacks del script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993ee413a0d20f9d",
   "metadata": {},
   "source": [
    "**Factor de expansión** -> Si se va demasiado, bajar --max-window a 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13781e223df865ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tok_train)/len(ds_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b664984b01679aa",
   "metadata": {},
   "source": [
    "**Sanity check** -> Comprobar que no hay basura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784f8f2c419dd617",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(tok_train[\"input_ids\"][0])[:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd307133236cf325",
   "metadata": {},
   "source": [
    "**Velocidad** -> Si el tiempo de ejecución es muy elevado usar --max-len 256 o subir --slide-stride a 192 para menos ventanas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
