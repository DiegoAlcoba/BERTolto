{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50b7964ea9a2b2da",
   "metadata": {},
   "source": [
    "R# Preprocesamiento de los comentarios para la primera fase de fine-tuning y entrenamiento del modelo\n",
    "En esta primera fase, una vez extraídos los bodies de todos los issues/PRs de los repositorios listados junto con 2 comentarios de 250 del total de issues de cada repositorio, se prepararán todos los datos para ser utilizados en el entrenamiento y ajuste fino del modelo que se utilizará.\n",
    "\n",
    "Se seguirán prácticamente los mismos pasos vistos en *c_preparing_data_for_statistics_and_ML* pero con varias diferencias claves que existen entre los modelos BERT que se utilizarán ahora y los modelos de clasificación presentados con anterioridad (notebooks de GVTIA).\n",
    "\n",
    "Para Transformers funciona mejor un preprocesado mínimo y dejar la segmentación al propio tokenizador del modelo, a continuación se muestra que procedimientos similares a los anteriores se mantendrán y cuáles se evitarán:\n",
    "\n",
    "## Se mantendrá:\n",
    "- Normalización de espacios/saltos de línea\n",
    "- Eliminación de caracteres de control raros o poco usuales\n",
    "- Se conservará el uso de mayúsculas y minúsculas, signos, números, URLs, nombres propios de vulnerabilidades o bugs (CVE-XXXX-YYYY), rutas (/etc/...), código entre backticks (`return salida`), nombres de APIs.\n",
    "- Se definirá una longitud máxima de tokens por comentario o el uso de *sliding window* si el texto es muy largo.\n",
    "\n",
    "## Se omitirá:\n",
    "- Pasar todo el texto a minúsculas, los modelos RoBERTa/DistilRoBERTa que se utilizarán utilizan mayúsculas y minúsculas.\n",
    "- Eliminar la puntuación y stopwords.\n",
    "- Stemming / lematización.\n",
    "- Normalizaciones agresivas de URLs/código -> se pierde señal técnica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decd1abca07ad89a",
   "metadata": {},
   "source": [
    "Una vez explicado esto, se comenzará con el preprocesado de todos los comentarios extraídos de GitHub, comenzando como se ha visto ya en diversas ocasiones, con cargar el documento (.csv) en un dataframe de pandas para su uso y manipulación.\n",
    "\n",
    "En este caso, se cuenta con 2 documentos:\n",
    "- **gh_bodys_lastyear.csv**. Archivo que contiene los bodies (comentario principal) de todos los Issues/PRs en el último año de los repositorios listados para la extracción de comentarios.\n",
    "- **gh_comments_lastyear.csv**. Archivo que contiene los 2 primeros comentarios de cada Issue/PR de 250 Issues/PRs por repositorio (500 comentarios por repo), en gran parte de los casos serán las respuestas aportadas por usuarios al body del documento anterior.\n",
    "\n",
    "En este caso, como se cuenta con 2 documentos lo que se hará es crear 2 dataframes, uno con cada documento, para a continuación unirlos con la función `concat()` de pandas y ordenarlos según el id del Issue/PR para la clara visualización y mantener una estructura coherente entre cuerpo principal y comentarios asociados."
   ]
  },
  {
   "cell_type": "code",
   "id": "3ed28d7ff902c74f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T22:47:12.957173Z",
     "start_time": "2025-10-28T22:47:11.166835Z"
    }
   },
   "source": [
    "import sys\n",
    "# Cargamos el archivo settings_bert.py que contiene todos los imports necesarios para la correcta ejecución del notebook y no tener que importarlos en cada celda de código\n",
    "%run -i ../notebooks_settings/settings_bert.py"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "3034471a4637f6e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T22:47:15.025195Z",
     "start_time": "2025-10-28T22:47:13.106862Z"
    }
   },
   "source": [
    "# Ruta de los archivos\n",
    "path_gh_comments = \"../data/gh_comments/training/gh_comments_2023_now.csv\"\n",
    "path_gh_comments = Path(path_gh_comments)\n",
    "\n",
    "EXPECTED_COLS = [\n",
    "    'repo','is_pr','issue_number','comment_type','comment_id','comment_created_at','comment_author',\n",
    "    'text','comment_url','context_id','container_title','container_state','container_url',\n",
    "    'container_created_at','container_updated_at','container_labels'\n",
    "]\n",
    "\n",
    "def read_with_header_fix(p: Path) -> pd.DataFrame:\n",
    "    # Se lee 1ª fila para inspeccionar columnas\n",
    "    probe = pd.read_csv(p, nrows=1)\n",
    "    if list(probe.columns) == EXPECTED_COLS:\n",
    "        return pd.read_csv(p)\n",
    "    # Si no coincide, reinterpretamos: no hay cabecera -> header=None + names=EXPECTED_COLS\n",
    "    return pd.read_csv(p, header=None, names=EXPECTED_COLS)\n",
    "\n",
    "df_gh = read_with_header_fix(path_gh_comments)\n",
    "\n",
    "# Tipos y ordenación\n",
    "df_gh['comment_created_at'] = pd.to_datetime(df_gh['comment_created_at'], errors='coerce', utc=True)\n",
    "df_gh.loc[df_gh['comment_created_at'].isna(), 'comment_created_at'] = pd.to_datetime(df_gh['container_created_at'], errors='coerce', utc=True)\n",
    "\n",
    "order_map = {'issue_body':0, 'pr_body':0} # Bodies primero -> coherencia\n",
    "df_gh['order'] = df_gh['comment_type'].map(order_map).fillna(1).astype(int)\n",
    "\n",
    "df_gh = df_gh.sort_values(by=['repo','issue_number','order','comment_created_at','comment_id'], kind='mergesort').drop(columns=['order'])\n",
    "\n",
    "# Normalizar booleano -> OPCIONAL\n",
    "df_gh['is_pr'] = df_gh['is_pr'].astype(str).str.lower().map({'true':True, 'false':False})\n",
    "\n",
    "len(df_gh)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100634"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "65043b29d28ed666",
   "metadata": {},
   "source": "Ahora sí están todos los comentarios bien ordenados."
  },
  {
   "cell_type": "markdown",
   "id": "17b3a603ab9a704c",
   "metadata": {},
   "source": [
    "Ahora sí se procederá al procesamiento del dataset para dejarlo preparado para el modelo BERT que se utilizará, RoBERTa o DistilRoBERTa. Este proceso se va a definir en una serie de scripts .py, cada uno con el objetivo de realizar una tarea para su reutilización en otros puntos del proyecto (cuando se haga el de reddit, u otros comentarios de github) de forma que estos sean agnósticos al sistema del que se extraen los comentarios que serán utilizados por el modelo.\n",
    "\n",
    "Del mismo modo, tras el procesamiento de los datos, el resultado del procesado será almacenado en archivos `.parquet` por su ligereza y agilidad a la hora de ser manipulados y consumidos por modelos BERT. Las principales ventajas de este formato son:\n",
    "- Más pequeño: compresión por columna, pesa de 2 a 5 veces menos que un `.csv`\n",
    "- Más rápido: lee solo las columnas que se necesitan (\"column pruning\") y aplica vectorización.\n",
    "- Conserva tipos: fechas, booleanos, enteros \"nullable\", etc. (`.csv`los pierde)\n",
    "- Esquema: guarda el _schema_ dentro del archivo -> menos sorpresas al cargarlo\n",
    "\n",
    "Por estas características el formato es el preferido para pipelines de datos/ML. En este caso la estructura que se utilizará será:\n",
    "- `merged.parquet` = será el dataset completo tras ingesta y normalización ligera.\n",
    "- `split_train.parquet`, `split_dev.parquet`, `split_test.parquet` = particiones del dataset ya divididas, listas para su tokenización.\n",
    "\n",
    "#### Ventajas frente a CSV\n",
    "- Evita problemas de comas y saltos de línea\n",
    "- Mantiene las fechas (`created_at`), booleanos (`is_pr`) y enteros sin perder el tipo.\n",
    "- Carga solo las columnas necesarias -> menor uso de RAM y tiempo de ejecución."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3d14442f2d3c47",
   "metadata": {},
   "source": [
    "## Ejecución del preprocesado de datos\n",
    "Se han definido varios scripts, cada uno con una función en el preprocesado de datos para modelos BERT, a  los cuáles se les llamará desde este notebook con los argumentos correspondientes para realizar este proceso.\n",
    "\n",
    "Las funciones reutilizables para el pipeline están definidas en `prep_utils.py`. Normaliza texto \"ligero\", mapea columnas heterogéneas (GH/Reddit) al esquema core (id, text, label, source, created_at, context_id), lee CSV/SQLite y guarda Parquet.\n",
    "El objetivo principal de estas funciones es su utilización cuando se desea agnosticismo de fuente (Github/Reddit) y un preprocesado mínimo ideal para Transformers."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Filtro de ruido (bots/plantilllas)",
   "id": "a3ac696769973105"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T22:47:17.070403Z",
     "start_time": "2025-10-28T22:47:15.039634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# prep_utils.py (función reutilizable)\n",
    "BOT_REGEX = r\"(bot$)|(\\[bot\\])|(github-actions)|(^k8s-ci-robot$)|(dependabot)|(^renovate)|(^mergify)\"\n",
    "TPL_PATTERNS = [\n",
    "    \"This issue is currently awaiting triage\",\n",
    "    \"Instructions for interacting with me using PR comments are available here\",\n",
    "]\n",
    "\n",
    "def is_noise(author: str, text: str) -> bool:\n",
    "    a = (author or \"\").lower()\n",
    "    t = (text or \"\").strip().lower()\n",
    "    if re.search(BOT_REGEX, a): return True\n",
    "    if len(t.split()) < 10: return True\n",
    "    for p in TPL_PATTERNS:\n",
    "        if p.lower() in t: return True\n",
    "    return False\n",
    "\n",
    "before = len(df_gh)\n",
    "df_gh = df_gh[~df_gh.apply(lambda r: is_noise(str(r.get(\"comment_author\",\"\")), str(r.get(\"text\",\"\"))), axis=1)].reset_index(drop=True)\n",
    "after = len(df_gh)\n",
    "\n",
    "print(f\"Filtrado ruido: {before-after} filas eliminadas / {after} restantes\")"
   ],
   "id": "d870348f90a89e1e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtrado ruido: 41439 filas eliminadas / 59195 restantes\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Construcción del esquema core (context, label, source, id, created_at)B",
   "id": "2b69c5131a907241"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Qué hace la celda\n",
    "\n",
    "1. **`build_context(row)`**\n",
    "\n",
    "   * Toma el **título** del issue/PR (`container_title`) y las **labels** (`container_labels`, separadas por `;`) y construye un campo de **contexto legible** para el modelo, con este formato:\n",
    "\n",
    "     ```\n",
    "     \"<título> [label1][label2][label3]\"\n",
    "     ```\n",
    "   * ¿Por qué? Ese “context” suele ayudar a los modelos de lenguaje a desambiguar el texto del comentario: el título resume el problema y las labels (p. ej. `sig/security`, `kind/bug`) añaden señal semántica muy útil.\n",
    "\n",
    "2. **`BUG_HINTS` y `SEC_HINTS`**\n",
    "\n",
    "   * Son conjuntos de **pistas** a nivel de label que indican, con heurística muy sencilla, si algo es “bug” o “security”. No es perfecto, pero vale para informes rápidos, estratificación de splits o para auditoría de cobertura.\n",
    "\n",
    "3. **`weak_label(row)`**\n",
    "\n",
    "   * Genera una **etiqueta débil** (`security`, `bug` u `other`) combinando:\n",
    "\n",
    "     * La intersección de las labels del issue/PR con `SEC_HINTS`/`BUG_HINTS`.\n",
    "     * Palabras clave en el **título** o el **texto** del comentario (p. ej. `cve-`, `vulnerability`, `rce`, `xss`, etc.).\n",
    "   * Útil para:\n",
    "\n",
    "     * Hacer **quick reports** (distribución por clases).\n",
    "     * **Filtrar**/priorizar subconjuntos.\n",
    "     * **Estratificar** splits train/dev/test manteniendo diversidad.\n",
    "   * Si no vas a usar clasificación supervisada por clase, puedes seguir dejándolo para informes y diagnóstico, o quitarlo.\n",
    "\n",
    "4. **Asignaciones al DataFrame**\n",
    "\n",
    "   * `df_gh[\"context\"]` → el contexto construido arriba.\n",
    "   * `df_gh[\"label\"]` → la etiqueta débil.\n",
    "   * `df_gh[\"source\"] = \"<GITHUB>\"` → columna con el **origen**.\n",
    "   * `df_gh[\"id\"] = df_gh[\"comment_id\"]` → un **ID estable** para deduplicar/trazar.\n",
    "   * `df_gh[\"created_at\"]` → fecha normalizada como `datetime` UTC (útil para splits temporales y análisis).\n",
    "\n",
    "5. **`cols_core` y `df_core`**\n",
    "\n",
    "   * Proyecta a un **esquema core** consistente: `id, text, context, label, source, created_at, …`\n",
    "   * Esto facilita reusar el mismo pipeline con otras fuentes en el futuro (o con nuevos extractores).\n",
    "\n",
    "---\n",
    "\n",
    "## ¿Por qué `source = \"<GITHUB>\"` si sólo usarás GitHub?\n",
    "\n",
    "* **Estabilidad de esquema**: dejar la columna `source` evita re-escribir scripts cuando añadas otra fuente (o distintas variantes de GitHub).\n",
    "* **Trazabilidad/reproducibilidad**: queda claro en los artefactos de datos de dónde provienen (útil en informes, auditoría y versionado).\n",
    "* **Prefijo de dominio opcional**: si en el futuro mezclas fuentes, puedes activar en tu tokenizador un **prefijo de dominio** (`--use-domain-prefix`) para enseñar al modelo a distinguir estilos de lenguaje. Con una sola fuente, **simplemente no lo activas** y el campo no molesta.\n",
    "\n",
    "> Si quieres, puedes quitarlo sin romper nada: elimina la línea que asigna `source`, y borra `\"source\"` de `cols_core`. Pero mi recomendación es **dejarlo**; no te penaliza y te ahorra cambios cuando amplíes el dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## Resumen rápido de utilidad de cada campo\n",
    "\n",
    "* **context**: aporta señal de alto nivel (título + labels) al modelo.\n",
    "* **label** (débil): para análisis/estratificación; no es obligatorio para el fine-tuning si no haces clasificación por clase.\n",
    "* **source**: trazabilidad y futura multi-fuente (inocuo si no lo usas).\n",
    "* **id**: dedupe + trazabilidad.\n",
    "* **created_at**: splits temporales y análisis por época.\n",
    "\n",
    "Si quieres, te paso una variante “minimal” sin `label` y sin `source`; pero tal cual está te da más control sin añadir complejidad real.\n"
   ],
   "id": "9d98b22352f4bfb6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T22:47:18.925921Z",
     "start_time": "2025-10-28T22:47:17.082598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- saneamiento previo para evitar NaN ---\n",
    "for col in [\"container_title\", \"container_labels\", \"text\", \"comment_author\"]:\n",
    "    if col not in df_gh.columns:\n",
    "        df_gh[col] = \"\"\n",
    "    df_gh[col] = df_gh[col].fillna(\"\").astype(str)\n",
    "\n",
    "def build_context(row) -> str:\n",
    "    title = str(row.get(\"container_title\") or \"\").strip()\n",
    "    labels_raw = row.get(\"container_labels\")\n",
    "    try:\n",
    "        labels = str(labels_raw).strip()\n",
    "    except Exception:\n",
    "        labels = \"\"\n",
    "    labels_list = [s for s in labels.split(\";\") if s]\n",
    "    labels_fmt = f\"[{']['.join(labels_list)}]\" if labels_list else \"\"\n",
    "    return \" \".join([title, labels_fmt]).strip()\n",
    "\n",
    "BUG_HINTS = {\"kind/bug\", \"sig/bug\", \"triage/accepted\", \"area/bug\"}\n",
    "SEC_HINTS = {\"sig/security\", \"area/security\", \"kind/security\"}\n",
    "\n",
    "def weak_label(row) -> str:\n",
    "    labs_str = str(row.get(\"container_labels\") or \"\")\n",
    "    labs = set(s.strip().lower() for s in labs_str.split(\";\") if s)\n",
    "    title = str(row.get(\"container_title\") or \"\").lower()\n",
    "    text  = str(row.get(\"text\") or \"\").lower()\n",
    "\n",
    "    if (labs & SEC_HINTS) or any(k in title or k in text for k in [\"cve-\", \"vulnerability\", \"exploit\", \"rce\", \"xss\", \"csrf\", \"ssrf\"]):\n",
    "        return \"security\"\n",
    "    if (labs & BUG_HINTS) or (\"bug\" in title):\n",
    "        return \"bug\"\n",
    "    return \"other\"\n",
    "\n",
    "df_gh[\"context\"] = df_gh.apply(build_context, axis=1)\n",
    "df_gh[\"label\"]   = df_gh.apply(weak_label, axis=1)\n",
    "df_gh[\"source\"]  = \"<GITHUB>\"\n",
    "df_gh[\"id\"]      = df_gh[\"comment_id\"]\n",
    "df_gh[\"created_at\"] = pd.to_datetime(df_gh[\"comment_created_at\"], errors=\"coerce\", utc=True)\n",
    "\n",
    "cols_core = [\n",
    "    \"id\",\"text\",\"context\",\"label\",\"source\",\"created_at\",\n",
    "    \"repo\",\"issue_number\",\"is_pr\",\"comment_type\",\"context_id\",\n",
    "    \"comment_author\",\"comment_url\",\"container_url\"\n",
    "]\n",
    "df_core = df_gh[cols_core].copy()\n",
    "df_core.head(3).T\n"
   ],
   "id": "3fb6b1e260095a88",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                                                                                                                                                      0  \\\n",
       "id                                                                                                                                              github_issuecomment_IC_kwDOAiVL486CLpLA   \n",
       "text            My code has one package.json in root and the other in release/app/package.json\\nI tried changing the version in package.json but it does not change the version number.   \n",
       "context                                                    extraResources get copied to the wrong folder on Linux/Windows [help wanted][invalid][question][readme improvement][backlog]   \n",
       "label                                                                                                                                                                          security   \n",
       "source                                                                                                                                                                         <GITHUB>   \n",
       "created_at                                                                                                                                                    2024-06-22 16:29:53+00:00   \n",
       "repo                                                                                                                                                 electron-userland/electron-builder   \n",
       "issue_number                                                                                                                                                                        379   \n",
       "is_pr                                                                                                                                                                             False   \n",
       "comment_type                                                                                                                                                              issue_comment   \n",
       "context_id                                                                                                                                 electron-userland/electron-builder#issue:379   \n",
       "comment_author                                                                                                                                                            rahulbansal16   \n",
       "comment_url                                                                                    https://github.com/electron-userland/electron-builder/issues/379#issuecomment-2184090304   \n",
       "container_url                                                                                                          https://github.com/electron-userland/electron-builder/issues/379   \n",
       "\n",
       "                                                                                                                                                                                                             1  \\\n",
       "id                                                                                                                                                                     github_issuecomment_IC_kwDOAiVL486ZLicv   \n",
       "text            From the discussion at develar/app-builder/issues/96 I gather this issue might not originate in electron-builder itself but rather in its dependency develar/app-builder.  Can anyone confirm?   \n",
       "context                                                                                                                                                                           RPM Failure [feature][linux]   \n",
       "label                                                                                                                                                                                                    other   \n",
       "source                                                                                                                                                                                                <GITHUB>   \n",
       "created_at                                                                                                                                                                           2025-01-03 23:43:01+00:00   \n",
       "repo                                                                                                                                                                        electron-userland/electron-builder   \n",
       "issue_number                                                                                                                                                                                               502   \n",
       "is_pr                                                                                                                                                                                                    False   \n",
       "comment_type                                                                                                                                                                                     issue_comment   \n",
       "context_id                                                                                                                                                        electron-userland/electron-builder#issue:502   \n",
       "comment_author                                                                                                                                                                                     logological   \n",
       "comment_url                                                                                                           https://github.com/electron-userland/electron-builder/issues/502#issuecomment-2569938735   \n",
       "container_url                                                                                                                                 https://github.com/electron-userland/electron-builder/issues/502   \n",
       "\n",
       "                                                                                                                                                                                                                      2  \n",
       "id                                                                                                                                                                              github_issuecomment_IC_kwDOAiVL485wtrJq  \n",
       "text            Hello everyone. I encountered this issue myself a few days ago and I've figured out how to fix it.\\nBasically, make sure any native files are not packed (not in the .asar file and instead in the a...  \n",
       "context                                                                                 ELECTRON_ASAR.js:158 Uncaught Error: The specified module could not be found. \\\\?\\C:\\Users\\HzPC\\AppData\\Local\\Temp\\937.tmp.node  \n",
       "label                                                                                                                                                                                                             other  \n",
       "source                                                                                                                                                                                                         <GITHUB>  \n",
       "created_at                                                                                                                                                                                    2024-01-14 18:07:56+00:00  \n",
       "repo                                                                                                                                                                                 electron-userland/electron-builder  \n",
       "issue_number                                                                                                                                                                                                        580  \n",
       "is_pr                                                                                                                                                                                                             False  \n",
       "comment_type                                                                                                                                                                                              issue_comment  \n",
       "context_id                                                                                                                                                                 electron-userland/electron-builder#issue:580  \n",
       "comment_author                                                                                                                                                                                                   Esinko  \n",
       "comment_url                                                                                                                    https://github.com/electron-userland/electron-builder/issues/580#issuecomment-1891021418  \n",
       "container_url                                                                                                                                          https://github.com/electron-userland/electron-builder/issues/580  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe tex2jax_ignore\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>github_issuecomment_IC_kwDOAiVL486CLpLA</td>\n",
       "      <td>github_issuecomment_IC_kwDOAiVL486ZLicv</td>\n",
       "      <td>github_issuecomment_IC_kwDOAiVL485wtrJq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>My code has one package.json in root and the other in release/app/package.json\\nI tried changing the version in package.json but it does not change the version number.</td>\n",
       "      <td>From the discussion at develar/app-builder/issues/96 I gather this issue might not originate in electron-builder itself but rather in its dependency develar/app-builder.  Can anyone confirm?</td>\n",
       "      <td>Hello everyone. I encountered this issue myself a few days ago and I've figured out how to fix it.\\nBasically, make sure any native files are not packed (not in the .asar file and instead in the a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context</th>\n",
       "      <td>extraResources get copied to the wrong folder on Linux/Windows [help wanted][invalid][question][readme improvement][backlog]</td>\n",
       "      <td>RPM Failure [feature][linux]</td>\n",
       "      <td>ELECTRON_ASAR.js:158 Uncaught Error: The specified module could not be found. \\\\?\\C:\\Users\\HzPC\\AppData\\Local\\Temp\\937.tmp.node</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>security</td>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source</th>\n",
       "      <td>&lt;GITHUB&gt;</td>\n",
       "      <td>&lt;GITHUB&gt;</td>\n",
       "      <td>&lt;GITHUB&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>created_at</th>\n",
       "      <td>2024-06-22 16:29:53+00:00</td>\n",
       "      <td>2025-01-03 23:43:01+00:00</td>\n",
       "      <td>2024-01-14 18:07:56+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>repo</th>\n",
       "      <td>electron-userland/electron-builder</td>\n",
       "      <td>electron-userland/electron-builder</td>\n",
       "      <td>electron-userland/electron-builder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>issue_number</th>\n",
       "      <td>379</td>\n",
       "      <td>502</td>\n",
       "      <td>580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_pr</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comment_type</th>\n",
       "      <td>issue_comment</td>\n",
       "      <td>issue_comment</td>\n",
       "      <td>issue_comment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context_id</th>\n",
       "      <td>electron-userland/electron-builder#issue:379</td>\n",
       "      <td>electron-userland/electron-builder#issue:502</td>\n",
       "      <td>electron-userland/electron-builder#issue:580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comment_author</th>\n",
       "      <td>rahulbansal16</td>\n",
       "      <td>logological</td>\n",
       "      <td>Esinko</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comment_url</th>\n",
       "      <td>https://github.com/electron-userland/electron-builder/issues/379#issuecomment-2184090304</td>\n",
       "      <td>https://github.com/electron-userland/electron-builder/issues/502#issuecomment-2569938735</td>\n",
       "      <td>https://github.com/electron-userland/electron-builder/issues/580#issuecomment-1891021418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>container_url</th>\n",
       "      <td>https://github.com/electron-userland/electron-builder/issues/379</td>\n",
       "      <td>https://github.com/electron-userland/electron-builder/issues/502</td>\n",
       "      <td>https://github.com/electron-userland/electron-builder/issues/580</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T22:47:29.739015Z",
     "start_time": "2025-10-28T22:47:18.975828Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(len(df_core))\n",
    "\n",
    "# Guarda SQLite (opcional)\n",
    "db_gh = \"../data/gh_comments/training/gh_dataset_2023-2025.db\"\n",
    "Path(db_gh).parent.mkdir(parents=True, exist_ok=True)\n",
    "con = sqlite3.connect(db_gh)\n",
    "df_core.to_sql('gh_dataset', con, if_exists='replace', index=False)\n",
    "con.close()\n",
    "\n",
    "# Guarda Parquet (entrada directa para el pipeline)\n",
    "merged_parquet = \"../src/artifacts/prep/merged.parquet\"\n",
    "Path(merged_parquet).parent.mkdir(parents=True, exist_ok=True)\n",
    "df_core.to_parquet(merged_parquet, index=False)\n",
    "print(\"Guardado:\", merged_parquet)\n"
   ],
   "id": "96947c81f8e6a4d7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59195\n",
      "Guardado: ../src/artifacts/prep/merged.parquet\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### `ingest_merge.py`\n",
    "Combina una o varias entradas en un único DataFrame, deduplica el contenido por `id`, aplica normalización ligera y guarda `merged.parquet` + meta. Se le da como input el archivo `.db` o los `.csv` deseados, produciendo en la salida un DF en formato `.parquet`."
   ],
   "id": "25430a09c3406432"
  },
  {
   "cell_type": "code",
   "id": "2087c680aa62567a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T22:47:46.192834Z",
     "start_time": "2025-10-28T22:47:29.757233Z"
    }
   },
   "source": [
    "base = \"../src/data_prep\"\n",
    "\n",
    "# 1. Ingesta + merge (de CSVs o db en SQLite)\n",
    "# Como ya tengo el dataset directamente almacenado en un .db\n",
    "subprocess.run([sys.executable, f\"{base}/ingest_merge.py\",\n",
    "                \"--sqlite-db\", \"../data/gh_comments/training/gh_dataset_2023-2025.db\",\n",
    "                \"--table\", \"gh_dataset\",\n",
    "                \"--apply-noise-filter\",\n",
    "                \"--build-core-schema\",\n",
    "                \"--drop-empty-text\",\n",
    "                \"--source-default\", \"<GITHUB>\",\n",
    "                \"--out-parquet\", \"../src/artifacts/prep/merged.parquet\",\n",
    "                \"--out-meta\", \"../src/artifacts/prep/merged_meta.json\"\n",
    "                ], check=True)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[drop-empty-text] 0 filas sin texto eliminadas\n",
      "OK -> ../src/artifacts/prep/merged.parquet\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['/home/diego/BERTolto/.venv/bin/python', '../src/data_prep/ingest_merge.py', '--sqlite-db', '../data/gh_comments/training/gh_dataset_2023-2025.db', '--table', 'gh_dataset', '--apply-noise-filter', '--build-core-schema', '--drop-empty-text', '--source-default', '<GITHUB>', '--out-parquet', '../src/artifacts/prep/merged.parquet', '--out-meta', '../src/artifacts/prep/merged_meta.json'], returncode=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "4f941dffedc431b4",
   "metadata": {},
   "source": [
    "### `quick_report.py`\n",
    "Muestra por pantalla un resumen del Parquet aportado en el input: nº de filas, distribución de `label`, distribución de `source`, rango de `created_at`.\n",
    "Es útil para la verificación visual de que la ingesta es correcta antes del split/tokenización e ideal para detectar desbalanceos fuertes o rangos temporales inesperados antes de la fase de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "id": "c054999514390b34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T22:47:46.868927Z",
     "start_time": "2025-10-28T22:47:46.252547Z"
    }
   },
   "source": [
    "# Informe rápido: añade labels/source + rango temporal + top repos\n",
    "subprocess.run([sys.executable, f\"{base}/quick_report.py\",\n",
    "                \"--in-parquet\", \"../src/artifacts/prep/merged.parquet\",\n",
    "                \"--topn\", \"12\"\n",
    "                ], check=True)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows: 59195\n",
      "labels: {'other': 31477, 'security': 19719, 'bug': 7999}\n",
      "fuentes: {'<GITHUB>': 59195}\n",
      "rango de fechas: 2023-01-01T03:34:12+00:00 -> 2025-10-24T18:44:34+00:00\n",
      "top 12 repos:\n",
      "  - grafana/grafana: 11743\n",
      "  - kubernetes/kubernetes: 10703\n",
      "  - vercel/next.js: 9726\n",
      "  - tensorflow/tensorflow: 7563\n",
      "  - nodejs/node: 4398\n",
      "  - pytorch/pytorch: 4358\n",
      "  - openssl/openssl: 3670\n",
      "  - envoyproxy/envoy: 3648\n",
      "  - prometheus/prometheus: 1426\n",
      "  - kubernetes/ingress-nginx: 1101\n",
      "  - electron-userland/electron-builder: 859\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['/home/diego/BERTolto/.venv/bin/python', '../src/data_prep/quick_report.py', '--in-parquet', '../src/artifacts/prep/merged.parquet', '--topn', '12'], returncode=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "cf0c11c11a32612a",
   "metadata": {},
   "source": [
    "### `split_thread_temporal.py`\n",
    "Divide el DF `merged.parquet` en train/dev/test respetando hilos (`context_id`) y orden temporal para evitar fuga de información entre splits manteniendo el orden cronológico dentro de cada uno.\n",
    "Los `ratios` son las proporciones con las que se dividirá el dataset:\n",
    "- train: 70%\n",
    "- dev: 15%\n",
    "- test: 15%"
   ]
  },
  {
   "cell_type": "code",
   "id": "4c116173457956d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T22:47:48.681181Z",
     "start_time": "2025-10-28T22:47:46.888125Z"
    }
   },
   "source": [
    "# Split temporal + thread-aware\n",
    "subprocess.run([sys.executable, f\"{base}/split_thread_temporal.py\",\n",
    "                \"--in-parquet\", \"../src/artifacts/prep/merged.parquet\",\n",
    "                \"--out-train\", \"../src/artifacts/prep/split_train.parquet\",\n",
    "                \"--out-dev\", \"../src/artifacts/prep/split_dev.parquet\",\n",
    "                \"--out-test\", \"../src/artifacts/prep/split_test.parquet\",\n",
    "                \"--ratios\", \"0.7\", \"0.15\", \"0.15\",\n",
    "                \"--out-meta\", \"../src/artifacts/prep/split_meta.json\"\n",
    "                ], check=True)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK -> ../src/artifacts/prep/split_train.parquet ../src/artifacts/prep/split_dev.parquet ../src/artifacts/prep/split_test.parquet\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['/home/diego/BERTolto/.venv/bin/python', '../src/data_prep/split_thread_temporal.py', '--in-parquet', '../src/artifacts/prep/merged.parquet', '--out-train', '../src/artifacts/prep/split_train.parquet', '--out-dev', '../src/artifacts/prep/split_dev.parquet', '--out-test', '../src/artifacts/prep/split_test.parquet', '--ratios', '0.7', '0.15', '0.15', '--out-meta', '../src/artifacts/prep/split_meta.json'], returncode=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "4e94cb88fd74d42c",
   "metadata": {},
   "source": [
    "### `tokenize_hf.py`\n",
    "Carga los Parquet de train/dev/test, antepone un prefijo de dominio (`<GITHUB>`, `<REDDIT>`, en este caso no porque todavía estoy solo con los comentarios de GH), tokeniza los textos con el tokenizador de DistilRoBERTa/RoBERTa y guarda:\n",
    "- `dataset/` (formato `save_to_disk` de HF Datasets)\n",
    "- `tokenizer/` (vocabulario + config)\n",
    "- `preprocess_meta.json` (tamaños, `max_len`, pesos de cada clase)\n",
    "\n",
    "#### Parámetros\n",
    "- `--base-model`: modelo que se utilizará, en este caso distilroberta\n",
    "- `--max-len 384`: longitud de secuencia; bajar acelera, subir captura más contexto.\n",
    "- `--use-domain-prefix`: etiqueta de la fuente (GITHUB/REDDIT) -> No se utiliza al usar solo github\n",
    "- `--sliding-window --slide-stride 128`: para textos muy largos (menos truncado)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17eb874affc5ab",
   "metadata": {},
   "source": [
    "Para decidir el `--max-len` y la posibilidad de utilizar `--sliding-window`, se puede calcular la distribución de longitudes en tokens y elegir el valor del argumento para cubrir el *p90-p95* y obtener un mejor resultado."
   ]
  },
  {
   "cell_type": "code",
   "id": "ae2900843d1dbb0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T22:48:44.918436Z",
     "start_time": "2025-10-28T22:47:48.699100Z"
    }
   },
   "source": [
    "tok = AutoTokenizer.from_pretrained(\"distilroberta-base\", use_fast=True)\n",
    "df = pd.read_parquet(\"../src/artifacts/prep/merged.parquet\") # o split_train.parquet\n",
    "lens = df[\"text\"].astype(str).map(lambda s: len(tok(s, truncation=False)[\"input_ids\"]))\n",
    "\n",
    "print(pd.Series(lens).describe(percentiles=[.5, .9, .95, .99]))"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1527 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    59195.00\n",
      "mean       732.62\n",
      "std       3020.39\n",
      "min         12.00\n",
      "50%        124.00\n",
      "90%       1022.00\n",
      "95%       2188.00\n",
      "99%      20218.90\n",
      "max     126155.00\n",
      "Name: text, dtype: float64\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "d790f7a5690dee84",
   "metadata": {},
   "source": [
    "### Argumentos del tokenizador\n",
    "- `--max-len 384`. Como el modelo solo puede leer hasta N tokens al mismo tiempo, se limita a N tokens (384) en la tokenización de cada texto. Si este tiene un nº superior se trunca o se divide en trozos (sliding-window). Elijo 384 tokens máx. por texto por su equilibrio en velocidad y uso de VRAM (no debería de tener problema con mi PC).\n",
    "- `--sliding-window`. Si un texto no cabe en `--max-len`, en lugar de descartar el sobrante se divide en ventanas solapadas, dando como resultado varias filas por cada texto largo.\n",
    "- `--slide-stride 128`. Es el número de tokens que se solapan entre ventanas para no cortar frases/palabras a la mitad cuando se divide un texto entre ventanas. En este caso si el texto se divide en 2 ventanas la distribución sería:\n",
    "    - Ventana 1: tokens 1-384\n",
    "    - Ventana 2: tokens (384-128) 256-639\n",
    "- `--max-windows-per-doc 6`. Limita a un máximo de 6 ventanas por texto, el resto se recorta. Es mejor que descartar el texto completo, y si supera el máx. de ventanas es muy probable que sea debido a mensajes de logs o diffs y no aporte nada al modelo, solo ruido.\n",
    "- `--filter-max-input-tokens`. Como se ve en la celda anterior (porcentajes) hay textos enormes y con este argumento se descartan sus colas limitando a N tokens antes de dividir en ventanas.\n",
    "- `--num-proc 12`. Usa 12 hilos de ejecución para map paralelo (Mi PC tiene 8C/16T) -> Implementar en el tokenizador\n",
    "- `--batch-chunk-size 512`. Indica el tamaño de trozos (grandes) en cada llamada al tokenizador. -> Implementar en el tokenizador\n",
    "\n",
    "#### Recomendaciones\n",
    "- Si ves que sube mucho el nº de ejemplos → baja --max-windows-per-doc a 4.\n",
    "- Si va justo de RAM → --num-proc 8 y/o --batch-chunk-size 256."
   ]
  },
  {
   "cell_type": "code",
   "id": "b048947f75cfae36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T22:58:40.776647Z",
     "start_time": "2025-10-28T22:57:31.922910Z"
    }
   },
   "source": [
    "env = dict(os.environ)\n",
    "\n",
    "# Fuerza UTF-8 en el hijo:\n",
    "env[\"PYTHONIOENCODING\"] = \"utf-8\"\n",
    "env[\"PYTHONUTF8\"] = \"1\"\n",
    "\n",
    "# Paralelización\n",
    "env[\"TOKENIZERS_PARALLELISM\"] = \"false\" #evita sub-proceso\n",
    "env[\"RAYON_NUM_THREADS\"] = \"2\" #2 hilos por proceso\n",
    "\n",
    "# tokenización HF (DistilRoBERTa/RoBERTa)\n",
    "cmd = [\n",
    "    sys.executable, f\"{base}/tokenize_hf.py\",\n",
    "    \"--train-parquet\", \"../src/artifacts/prep/split_train.parquet\",\n",
    "    \"--dev-parquet\",   \"../src/artifacts/prep/split_dev.parquet\",\n",
    "    \"--test-parquet\",  \"../src/artifacts/prep/split_test.parquet\",\n",
    "    \"--out-dir\", \"../src/artifacts/hf_distilroberta\",\n",
    "    \"--base-model\", \"distilroberta-base\",\n",
    "    \"--max-len\", \"384\",\n",
    "    \"--sliding-window\",\n",
    "    \"--slide-stride\", \"128\",\n",
    "    \"--max-window-per-doc\", \"8\",      # tu script acepta ambas variantes\n",
    "    \"--filter-max-input-tokens\", \"8192\",\n",
    "    \"--num-proc\", \"8\",\n",
    "    \"--batch-chunk-size\", \"512\",\n",
    "]\n",
    "\n",
    "res = subprocess.run(cmd, env=env, capture_output=True, text=True)\n",
    "print(\"---- STDOUT ----\\n\", res.stdout)\n",
    "print(\"---- STDERR ----\\n\", res.stderr)\n",
    "if res.returncode != 0:\n",
    "    raise RuntimeError(f\"tokenize_hf.py falló con exit code {res.returncode}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- STDOUT ----\n",
      " [filtro] train -> 725 descartados por > 8192 tokens\n",
      "[filtro] validation -> 151 descartados por > 8192 tokens\n",
      "[filtro] test -> 161 descartados por > 8192 tokens\n",
      "OK -> ../src/artifacts/hf_distilroberta/dataset ../src/artifacts/hf_distilroberta/tokenizer\n",
      "\n",
      "---- STDERR ----\n",
      " None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1142 > 384). Running this sequence through the model will result in indexing errors\n",
      "\n",
      "Map (num_proc=8):   0%|          | 0/40719 [00:00<?, ? examples/s]\n",
      "Map (num_proc=8):   1%|▏         | 512/40719 [00:01<01:37, 414.01 examples/s]\n",
      "Map (num_proc=8):   3%|▎         | 1024/40719 [00:01<00:45, 876.74 examples/s]\n",
      "Map (num_proc=8):   4%|▍         | 1536/40719 [00:01<00:28, 1371.36 examples/s]\n",
      "Map (num_proc=8):   6%|▋         | 2560/40719 [00:01<00:16, 2370.21 examples/s]\n",
      "Map (num_proc=8):   8%|▊         | 3072/40719 [00:01<00:14, 2595.34 examples/s]\n",
      "Map (num_proc=8):  11%|█▏        | 4608/40719 [00:01<00:08, 4382.51 examples/s]\n",
      "Map (num_proc=8):  14%|█▍        | 5632/40719 [00:02<00:07, 4772.95 examples/s]\n",
      "Map (num_proc=8):  19%|█▉        | 7680/40719 [00:02<00:04, 7403.88 examples/s]\n",
      "Map (num_proc=8):  23%|██▎       | 9216/40719 [00:02<00:04, 6629.19 examples/s]\n",
      "Map (num_proc=8):  28%|██▊       | 11264/40719 [00:02<00:03, 8874.27 examples/s]\n",
      "Map (num_proc=8):  33%|███▎      | 13312/40719 [00:02<00:03, 8253.58 examples/s]\n",
      "Map (num_proc=8):  39%|███▉      | 15872/40719 [00:03<00:02, 10464.63 examples/s]\n",
      "Map (num_proc=8):  43%|████▎     | 17408/40719 [00:03<00:02, 9547.38 examples/s] \n",
      "Map (num_proc=8):  47%|████▋     | 18944/40719 [00:03<00:02, 10552.71 examples/s]\n",
      "Map (num_proc=8):  52%|█████▏    | 20992/40719 [00:03<00:01, 10656.80 examples/s]\n",
      "Map (num_proc=8):  57%|█████▋    | 23040/40719 [00:03<00:01, 11993.10 examples/s]\n",
      "Map (num_proc=8):  60%|██████    | 24576/40719 [00:03<00:01, 11010.95 examples/s]\n",
      "Map (num_proc=8):  64%|██████▍   | 26112/40719 [00:04<00:01, 11266.71 examples/s]\n",
      "Map (num_proc=8):  68%|██████▊   | 27648/40719 [00:04<00:01, 10755.97 examples/s]\n",
      "Map (num_proc=8):  72%|███████▏  | 29184/40719 [00:04<00:01, 10966.81 examples/s]\n",
      "Map (num_proc=8):  78%|███████▊  | 31714/40719 [00:04<00:00, 10496.79 examples/s]\n",
      "Map (num_proc=8):  82%|████████▏ | 33220/40719 [00:04<00:00, 10680.27 examples/s]\n",
      "Map (num_proc=8):  87%|████████▋ | 35238/40719 [00:04<00:00, 9344.00 examples/s] \n",
      "Map (num_proc=8):  90%|█████████ | 36774/40719 [00:05<00:00, 9795.68 examples/s]\n",
      "Map (num_proc=8):  94%|█████████▍| 38280/40719 [00:05<00:00, 8660.13 examples/s]\n",
      "Map (num_proc=8):  96%|█████████▋| 39244/40719 [00:05<00:00, 8711.45 examples/s]\n",
      "Map (num_proc=8): 100%|██████████| 40719/40719 [00:05<00:00, 8900.72 examples/s]\n",
      "Map (num_proc=8): 100%|██████████| 40719/40719 [00:05<00:00, 7029.47 examples/s]\n",
      "\n",
      "Map (num_proc=8):   0%|          | 0/8736 [00:00<?, ? examples/s]\n",
      "Map (num_proc=8):   6%|▌         | 512/8736 [00:01<00:19, 418.13 examples/s]\n",
      "Map (num_proc=8):  12%|█▏        | 1024/8736 [00:01<00:09, 820.49 examples/s]\n",
      "Map (num_proc=8):  47%|████▋     | 4096/8736 [00:01<00:01, 4223.61 examples/s]\n",
      "Map (num_proc=8):  64%|██████▍   | 5632/8736 [00:01<00:00, 4551.24 examples/s]\n",
      "Map (num_proc=8):  85%|████████▌ | 7440/8736 [00:01<00:00, 6116.60 examples/s]\n",
      "Map (num_proc=8): 100%|██████████| 8736/8736 [00:02<00:00, 6964.90 examples/s]\n",
      "Map (num_proc=8): 100%|██████████| 8736/8736 [00:02<00:00, 3927.90 examples/s]\n",
      "\n",
      "Map (num_proc=8):   0%|          | 0/8703 [00:00<?, ? examples/s]\n",
      "Map (num_proc=8):   6%|▌         | 512/8703 [00:01<00:17, 481.58 examples/s]\n",
      "Map (num_proc=8):  12%|█▏        | 1024/8703 [00:01<00:07, 994.15 examples/s]\n",
      "Map (num_proc=8):  18%|█▊        | 1536/8703 [00:01<00:04, 1474.35 examples/s]\n",
      "Map (num_proc=8):  41%|████      | 3584/8703 [00:01<00:01, 4276.48 examples/s]\n",
      "Map (num_proc=8):  59%|█████▉    | 5120/8703 [00:01<00:00, 5818.62 examples/s]\n",
      "Map (num_proc=8):  73%|███████▎  | 6335/8703 [00:01<00:00, 6411.74 examples/s]\n",
      "Map (num_proc=8):  86%|████████▌ | 7487/8703 [00:01<00:00, 7064.54 examples/s]\n",
      "Map (num_proc=8): 100%|██████████| 8703/8703 [00:02<00:00, 7966.86 examples/s]\n",
      "Map (num_proc=8): 100%|██████████| 8703/8703 [00:02<00:00, 4082.08 examples/s]\n",
      "\n",
      "Saving the dataset (0/1 shards):   0%|          | 0/66831 [00:00<?, ? examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 66831/66831 [00:00<00:00, 256827.06 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 66831/66831 [00:00<00:00, 256638.01 examples/s]\n",
      "\n",
      "Saving the dataset (0/1 shards):   0%|          | 0/16111 [00:00<?, ? examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 16111/16111 [00:00<00:00, 285153.06 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 16111/16111 [00:00<00:00, 284540.70 examples/s]\n",
      "\n",
      "Saving the dataset (0/1 shards):   0%|          | 0/15077 [00:00<?, ? examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 15077/15077 [00:00<00:00, 272239.92 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 15077/15077 [00:00<00:00, 271353.26 examples/s]\n",
      "\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "993ee413a0d20f9d",
   "metadata": {},
   "source": "Cálculo del factor de expansión (filas tokenizadas / filas originales) -> Si se va demasiado, bajar --max-window a 4"
  },
  {
   "cell_type": "code",
   "id": "13781e223df865ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T06:45:45.708646Z",
     "start_time": "2025-10-29T06:45:45.191080Z"
    }
   },
   "source": [
    "# Carga tamaños \"antes\" (parquets de split)\n",
    "split_train_path = \"../src/artifacts/prep/split_train.parquet\"\n",
    "split_dev_path   = \"../src/artifacts/prep/split_dev.parquet\"\n",
    "split_test_path  = \"../src/artifacts/prep/split_test.parquet\"\n",
    "\n",
    "n_train_raw = len(pd.read_parquet(split_train_path))\n",
    "n_dev_raw   = len(pd.read_parquet(split_dev_path))\n",
    "n_test_raw  = len(pd.read_parquet(split_test_path))\n",
    "\n",
    "# Carga dataset tokenizado \"después\"\n",
    "from datasets import load_from_disk\n",
    "tok_ds = load_from_disk(\"../src/artifacts/hf_distilroberta/dataset\")\n",
    "\n",
    "n_train_tok = len(tok_ds[\"train\"])\n",
    "n_dev_tok   = len(tok_ds[\"validation\"])\n",
    "n_test_tok  = len(tok_ds[\"test\"])\n",
    "\n",
    "exp_train = n_train_tok / n_train_raw\n",
    "exp_dev   = n_dev_tok   / n_dev_raw\n",
    "exp_test  = n_test_tok  / n_test_raw\n",
    "\n",
    "print(f\"Train: {n_train_raw} -> {n_train_tok} (x{exp_train:.2f})\")\n",
    "print(f\"Dev:   {n_dev_raw} -> {n_dev_tok} (x{exp_dev:.2f})\")\n",
    "print(f\"Test:  {n_test_raw} -> {n_test_tok} (x{exp_test:.2f})\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 41444 -> 66831 (x1.61)\n",
      "Dev:   8887 -> 16111 (x1.81)\n",
      "Test:  8864 -> 15077 (x1.70)\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "1b664984b01679aa",
   "metadata": {},
   "source": [
    "**Sanity check** -> Comprobar que no hay basura\n",
    "Decodificación de un ejemplo tokenizado"
   ]
  },
  {
   "cell_type": "code",
   "id": "784f8f2c419dd617",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T06:47:06.076197Z",
     "start_time": "2025-10-29T06:47:05.980168Z"
    }
   },
   "source": [
    "# Carga tokenizer guardado (el del artefacto, no el de huggingface por nombre)\n",
    "tok = AutoTokenizer.from_pretrained(\"../src/artifacts/hf_distilroberta/tokenizer\", use_fast=True)\n",
    "\n",
    "# Ejemplo del split de train ya tokenizado\n",
    "ex = tok_ds[\"train\"][0]\n",
    "decoded = tok.decode(ex[\"input_ids\"], skip_special_tokens=True)\n",
    "print(decoded[:500])\n",
    "\n",
    "# Opcional: ver metadatos (mappings de labels si los guardaste)\n",
    "meta_path = Path(\"../src/artifacts/hf_distilroberta/preprocess_meta.json\")\n",
    "if meta_path.exists():\n",
    "    meta = json.loads(meta_path.read_text(encoding=\"utf-8\"))\n",
    "    print(\"\\nMeta breve:\", {k: meta[k] for k in [\"base_model\",\"max_len\",\"sliding_window\",\"slide_stride\",\"counts\"]})\n",
    "    if \"label_to_id\" in meta and meta[\"label_to_id\"]:\n",
    "        print(\"label_to_id:\", meta[\"label_to_id\"])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verify canary release\n",
      "\n",
      " I verified that the issue exists in the latest Next.js canary release\n",
      "\n",
      "Provide environment information\n",
      "Operating System:\n",
      " Platform: linux\n",
      " Arch: x64\n",
      " Version: #62-Ubuntu SMP Tue Nov 22 19:54:14 UTC 2022\n",
      "Binaries:\n",
      " Node: 16.19.0\n",
      " npm: 8.19.3\n",
      " Yarn: N/A\n",
      " pnpm: 7.18.2\n",
      "Relevant packages:\n",
      " next: 13.1.1\n",
      " eslint-config-next: 13.1.1\n",
      " react: 18.2.0\n",
      " react-dom: 18.2.0\n",
      "\n",
      "Which area(s) of Next.js are affected? (leave empty if unsure)\n",
      "App directory (appDir: true)\n",
      "Link to the code that \n",
      "\n",
      "Meta breve: {'base_model': 'distilroberta-base', 'max_len': 384, 'sliding_window': True, 'slide_stride': 128, 'counts': {'train': 66831, 'validation': 16111, 'test': 15077}}\n",
      "label_to_id: {'bug': 0, 'other': 1, 'security': 2}\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "cd307133236cf325",
   "metadata": {},
   "source": [
    "**Velocidad** -> Si el tiempo de ejecución es muy elevado usar --max-len 256 o subir --slide-stride a 192 para menos ventanas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
