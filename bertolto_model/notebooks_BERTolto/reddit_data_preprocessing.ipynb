{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50b7964ea9a2b2da",
   "metadata": {},
   "source": [
    "# Preprocesamiento de los comentarios para la primera fase de fine-tuning y entrenamiento del modelo\n",
    "En esta primera fase, una vez extraídos los bodies de todos los issues/PRs de los repositorios listados junto con 2 comentarios de 250 del total de issues de cada repositorio, se prepararán todos los datos para ser utilizados en el entrenamiento y ajuste fino del modelo que se utilizará.\n",
    "\n",
    "Se seguirán prácticamente los mismos pasos vistos en *c_preparing_data_for_statistics_and_ML* pero con varias diferencias claves que existen entre los modelos BERT que se utilizarán ahora y los modelos de clasificación presentados con anterioridad (notebooks de GVTIA).\n",
    "\n",
    "Para Transformers funciona mejor un preprocesado mínimo y dejar la segmentación al propio tokenizador del modelo, a continuación se muestra que procedimientos similares a los anteriores se mantendrán y cuáles se evitarán:\n",
    "\n",
    "## Se mantendrá:\n",
    "- Normalización de espacios/saltos de línea\n",
    "- Eliminación de caracteres de control raros o poco usuales\n",
    "- Se conservará el uso de mayúsculas y minúsculas, signos, números, URLs, nombres propios de vulnerabilidades o bugs (CVE-XXXX-YYYY), rutas (/etc/...), código entre backticks (`return salida`), nombres de APIs.\n",
    "- Se definirá una longitud máxima de tokens por comentario o el uso de *sliding window* si el texto es muy largo.\n",
    "\n",
    "## Se omitirá:\n",
    "- Pasar todo el texto a minúsculas, los modelos RoBERTa/DistilRoBERTa que se utilizarán utilizan mayúsculas y minúsculas.\n",
    "- Eliminar la puntuación y stopwords.\n",
    "- Stemming / lematización.\n",
    "- Normalizaciones agresivas de URLs/código -> se pierde señal técnica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decd1abca07ad89a",
   "metadata": {},
   "source": [
    "Una vez explicado esto, se comenzará con el preprocesado de todos los comentarios extraídos de GitHub, comenzando como se ha visto ya en diversas ocasiones, con cargar el documento (.csv) en un dataframe de pandas para su uso y manipulación.\n",
    "\n",
    "En este caso, se cuenta con 2 documentos:\n",
    "- **gh_bodys_lastyear.csv**. Archivo que contiene los bodies (comentario principal) de todos los Issues/PRs en el último año de los repositorios listados para la extracción de comentarios.\n",
    "- **gh_comments_lastyear.csv**. Archivo que contiene los 2 primeros comentarios de cada Issue/PR de 250 Issues/PRs por repositorio (500 comentarios por repo), en gran parte de los casos serán las respuestas aportadas por usuarios al body del documento anterior.\n",
    "\n",
    "En este caso, como se cuenta con 2 documentos lo que se hará es crear 2 dataframes, uno con cada documento, para a continuación unirlos con la función `concat()` de pandas y ordenarlos según el id del Issue/PR para la clara visualización y mantener una estructura coherente entre cuerpo principal y comentarios asociados."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T19:09:38.793053Z",
     "start_time": "2025-10-21T19:09:38.785265Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === Bootstrap cachés HF y PIP en ruta del proyecto ===\n",
    "import os, pathlib\n",
    "\n",
    "PROJ = pathlib.Path.home() / \"BERTolto\"\n",
    "HF_HOME = PROJ / \".hf_home\"\n",
    "PIP_CACHE = PROJ / \".cache\" / \"pip\"\n",
    "\n",
    "os.environ.setdefault(\"HF_HOME\", str(HF_HOME))\n",
    "os.environ.setdefault(\"HUGGINGFACE_HUB_CACHE\", str(HF_HOME / \"hub\"))\n",
    "os.environ.setdefault(\"TRANSFORMERS_CACHE\", str(HF_HOME / \"transformers\"))\n",
    "os.environ.setdefault(\"HF_DATASETS_CACHE\", str(HF_HOME / \"datasets\"))\n",
    "os.environ.setdefault(\"PIP_CACHE_DIR\", str(PIP_CACHE))\n",
    "os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
    "os.environ.setdefault(\"HF_HUB_DISABLE_TELEMETRY\", \"1\")\n",
    "# (En entornos de \"solo tokenizador\", evitar intentar TF/JAX)\n",
    "os.environ.setdefault(\"USE_TF\", \"0\")\n",
    "os.environ.setdefault(\"USE_JAX\", \"0\")\n",
    "\n",
    "for p in (HF_HOME, HF_HOME/\"hub\", HF_HOME/\"transformers\", HF_HOME/\"datasets\", PIP_CACHE):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"HF_HOME:\", os.environ[\"HF_HOME\"])\n",
    "print(\"TRANSFORMERS_CACHE:\", os.environ[\"TRANSFORMERS_CACHE\"])\n",
    "print(\"HF_DATASETS_CACHE:\", os.environ[\"HF_DATASETS_CACHE\"])\n",
    "print(\"PIP_CACHE_DIR:\", os.environ[\"PIP_CACHE_DIR\"])"
   ],
   "id": "ea43693ed205ae66",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_HOME: /home/diego/BERTolto/.hf_home\n",
      "TRANSFORMERS_CACHE: /home/diego/BERTolto/.hf_home/transformers\n",
      "HF_DATASETS_CACHE: /home/diego/BERTolto/.hf_home/datasets\n",
      "PIP_CACHE_DIR: /home/diego/BERTolto/.cache/pip\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "3ed28d7ff902c74f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T19:09:40.924281Z",
     "start_time": "2025-10-21T19:09:38.943523Z"
    }
   },
   "source": [
    "import os\n",
    "import subprocess\n",
    "# Cargamos el archivo settings_bert.py que contiene todos los imports necesarios para la correcta ejecución del notebook y no tener que importarlos en cada celda de código\n",
    "%run -i ../settings_bert.py"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T19:09:41.519403Z",
     "start_time": "2025-10-21T19:09:40.932143Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# RUTA del CSV de Reddit (tal como lo has scrapeado)\n",
    "path_reddit_comments = \"../data/reddit_comments/training/comentarios_reddit_raw_lastyear.csv\"\n",
    "\n",
    "# 1) Cargar\n",
    "df_re = pd.read_csv(path_reddit_comments)\n",
    "\n",
    "# 2) Normalizar nombres de columnas -> a lo que esperan nuestros scripts genéricos\n",
    "#    - text      <- comment_body\n",
    "#    - created_utc <- comment_created_utc   (prep_utils reconocerá created_utc)\n",
    "#    - link_id   <- submission_id           (prep_utils lo usa para context_id)\n",
    "#    - url       <- comment_permalink       (sirve para inferir source=reddit)\n",
    "rename_map = {\n",
    "    \"comment_body\": \"text\",\n",
    "    \"comment_created_utc\": \"created_utc\",\n",
    "    \"submission_id\": \"link_id\",\n",
    "    \"comment_permalink\": \"url\",\n",
    "}\n",
    "df_re = df_re.rename(columns={k:v for k,v in rename_map.items() if k in df_re.columns})\n",
    "\n",
    "# 3) (Opcional pero recomendable) sello de fuente explícito\n",
    "df_re[\"source\"] = \"reddit\"\n",
    "\n",
    "# 4) Guardar CSV temporal ya “amigable”\n",
    "tmp_norm_csv = \"../data/reddit_comments/training/comentarios_reddit_norm.csv\"\n",
    "Path(tmp_norm_csv).parent.mkdir(parents=True, exist_ok=True)\n",
    "df_re.to_csv(tmp_norm_csv, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"OK. Columnas normalizadas y CSV temporal guardado en:\", tmp_norm_csv)\n",
    "print(\"Cols:\", df_re.columns.tolist()[:20])\n",
    "print(\"Filas:\", len(df_re))"
   ],
   "id": "e57d08255ba9c6cc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK. Columnas normalizadas y CSV temporal guardado en: ../data/reddit_comments/training/comentarios_reddit_norm.csv\n",
      "Cols: ['subreddit', 'link_id', 'submission_created_utc', 'submission_title', 'comment_id', 'created_utc', 'comment_author', 'text', 'url', 'submission_url', 'source']\n",
      "Filas: 31335\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "de15d1140cc5ed4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T19:09:41.847047Z",
     "start_time": "2025-10-21T19:09:41.530826Z"
    }
   },
   "source": [
    "# OPCIONAL: guardar también en SQLite (tabla nueva) por trazabilidad\n",
    "db_re = \"../data/reddit_comments/training/reddit_dataset_lastyear.db\"\n",
    "con = sqlite3.connect(db_re)\n",
    "df_re.to_sql('reddit_dataset', con, if_exists='replace', index=False)\n",
    "con.close()\n",
    "print(\"Reddit guardado en SQLite:\", db_re, \"tabla=reddit_dataset\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reddit guardado en SQLite: ../data/reddit_comments/training/reddit_dataset_lastyear.db tabla=reddit_dataset\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "17b3a603ab9a704c",
   "metadata": {},
   "source": [
    "Ahora sí se procederá al procesamiento del dataset para dejarlo preparado para el modelo BERT que se utilizará, RoBERTa o DistilRoBERTa. Este proceso se va a definir en una serie de scripts .py, cada uno con el objetivo de realizar una tarea para su reutilización en otros puntos del proyecto (cuando se haga el de reddit, u otros comentarios de github) de forma que estos sean agnósticos al sistema del que se extraen los comentarios que serán utilizados por el modelo.\n",
    "\n",
    "Del mismo modo, tras el procesamiento de los datos, el resultado del procesado será almacenado en archivos `.parquet` por su ligereza y agilidad a la hora de ser manipulados y consumidos por modelos BERT. Las principales ventajas de este formato son:\n",
    "- Más pequeño: compresión por columna, pesa de 2 a 5 veces menos que un `.csv`\n",
    "- Más rápido: lee solo las columnas que se necesitan (\"column pruning\") y aplica vectorización.\n",
    "- Conserva tipos: fechas, booleanos, enteros \"nullable\", etc. (`.csv`los pierde)\n",
    "- Esquema: guarda el _schema_ dentro del archivo -> menos sorpresas al cargarlo\n",
    "\n",
    "Por estas características el formato es el preferido para pipelines de datos/ML. En este caso la estructura que se utilizará será:\n",
    "- `merged.parquet` = será el dataset completo tras ingesta y normalización ligera.\n",
    "- `split_train.parquet`, `split_dev.parquet`, `split_test.parquet` = particiones del dataset ya divididas, listas para su tokenización.\n",
    "\n",
    "#### Ventajas frente a CSV\n",
    "- Evita problemas de comas y saltos de línea\n",
    "- Mantiene las fechas (`created_at`), booleanos (`is_pr`) y enteros sin perder el tipo.\n",
    "- Carga solo las columnas necesarias -> menor uso de RAM y tiempo de ejecución."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3d14442f2d3c47",
   "metadata": {},
   "source": [
    "## Ejecución del preprocesado de datos\n",
    "Se han definido varios scripts, cada uno con una función en el preprocesado de datos para modelos BERT, a  los cuáles se les llamará desde este notebook con los argumentos correspondientes para realizar este proceso.\n",
    "\n",
    "Las funciones reutilizables para el pipeline están definidas en `prep_utils.py`. Normaliza texto \"ligero\", mapea columnas heterogéneas (GH/Reddit) al esquema core (id, text, label, source, created_at, context_id), lee CSV/SQLite y guarda Parquet.\n",
    "El objetivo principal de estas funciones es su utilización cuando se desea agnosticismo de fuente (Github/Reddit) y un preprocesado mínimo ideal para Transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25430a09c3406432",
   "metadata": {},
   "source": [
    "### `ingest_merge.py`\n",
    "Combina una o varias entradas en un único DataFrame, deduplica el contenido por `id`, aplica normalización ligera y guarda `merged.parquet` + meta. Se le da como input el archivo `.csv`, produciendo en la salida un DF en formato `.parquet`."
   ]
  },
  {
   "cell_type": "code",
   "id": "2087c680aa62567a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T19:09:43.249826Z",
     "start_time": "2025-10-21T19:09:41.857692Z"
    }
   },
   "source": [
    "base = \"../src/data_prep\"\n",
    "\n",
    "# 1) Ingesta + merge (desde CSV normalizado Reddit)\n",
    "subprocess.run([\n",
    "    sys.executable, f\"{base}/ingest_merge.py\",\n",
    "    \"--inputs\", tmp_norm_csv,                         # <<< CSV Reddit normalizado\n",
    "    \"--out-parquet\", \"../src/artifacts_rd/prep/merged_reddit.parquet\",\n",
    "    \"--out-meta\",    \"../src/artifacts_rd/prep/merged_meta_reddit.json\"\n",
    "], check=True)\n",
    "\n",
    "print(\"OK -> merged_reddit.parquet\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK -> ../src/artifacts_rd/prep/merged_reddit.parquet\n",
      "OK -> merged_reddit.parquet\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "4f941dffedc431b4",
   "metadata": {},
   "source": [
    "### `quick_report.py`\n",
    "Muestra por pantalla un resumen del Parquet aportado en el input: nº de filas, distribución de `label`, distribución de `source`, rango de `created_at`.\n",
    "Es útil para la verificación visual de que la ingesta es correcta antes del split/tokenización e ideal para detectar desbalanceos fuertes o rangos temporales inesperados antes de la fase de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "id": "c054999514390b34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T19:09:43.663892Z",
     "start_time": "2025-10-21T19:09:43.261773Z"
    }
   },
   "source": [
    "subprocess.run([\n",
    "    sys.executable, f\"{base}/quick_report.py\",\n",
    "    \"--in-parquet\", \"../src/artifacts_rd/prep/merged_reddit.parquet\"\n",
    "], check=True)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows: 31335\n",
      "labels: {0: 31335}\n",
      "fuentes: {'reddit': 31335}\n",
      "rango de fechas: 2024-10-19 18:21:04+00:00 -> 2025-10-19 10:42:38+00:00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['/home/diego/BERTolto/.venv/bin/python', '../src/data_prep/quick_report.py', '--in-parquet', '../src/artifacts_rd/prep/merged_reddit.parquet'], returncode=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "cf0c11c11a32612a",
   "metadata": {},
   "source": [
    "### `split_thread_temporal.py`\n",
    "Divide el DF `merged.parquet` en train/dev/test respetando hilos (`context_id`) y orden temporal para evitar fuga de información entre splits manteniendo el orden cronológico dentro de cada uno.\n",
    "Los `ratios` son las proporciones con las que se dividirá el dataset:\n",
    "- train: 70%\n",
    "- dev: 15%\n",
    "- test: 15%"
   ]
  },
  {
   "cell_type": "code",
   "id": "4c116173457956d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T19:09:44.223634Z",
     "start_time": "2025-10-21T19:09:43.676052Z"
    }
   },
   "source": [
    "subprocess.run([\n",
    "    sys.executable, f\"{base}/split_thread_temporal.py\",\n",
    "    \"--in-parquet\", \"../src/artifacts_rd/prep/merged_reddit.parquet\",\n",
    "    \"--out-train\",  \"../src/artifacts_rd/prep/split_train_reddit.parquet\",\n",
    "    \"--out-dev\",    \"../src/artifacts_rd/prep/split_dev_reddit.parquet\",\n",
    "    \"--out-test\",   \"../src/artifacts_rd/prep/split_test_reddit.parquet\",\n",
    "    \"--ratios\",     \"0.7\", \"0.15\", \"0.15\",\n",
    "    \"--out-meta\",   \"../src/artifacts_rd/prep/split_meta_reddit.json\"\n",
    "], check=True)\n",
    "\n",
    "print(\"OK -> splits reddit\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK -> ../src/artifacts_rd/prep/split_train_reddit.parquet ../src/artifacts_rd/prep/split_dev_reddit.parquet ../src/artifacts_rd/prep/split_test_reddit.parquet\n",
      "OK -> splits reddit\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "4e94cb88fd74d42c",
   "metadata": {},
   "source": [
    "### `tokenize_hf.py`\n",
    "Carga los Parquet de train/dev/test, antepone un prefijo de dominio (`<GITHUB>`, `<REDDIT>`, en este caso no porque todavía estoy solo con los comentarios de GH), tokeniza los textos con el tokenizador de DistilRoBERTa/RoBERTa y guarda:\n",
    "- `dataset/` (formato `save_to_disk` de HF Datasets)\n",
    "- `tokenizer/` (vocabulario + config)\n",
    "- `preprocess_meta.json` (tamaños, `max_len`, pesos de cada clase)\n",
    "\n",
    "#### Parámetros\n",
    "- `--base-model`: modelo que se utilizará, en este caso distilroberta\n",
    "- `--max-len 384`: longitud de secuencia; bajar acelera, subir captura más contexto.\n",
    "- `--use-domain-prefix`: etiqueta de la fuente (GITHUB/REDDIT)\n",
    "- `--sliding-window --slide-stride 128`: para textos muy largos (menos truncado)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17eb874affc5ab",
   "metadata": {},
   "source": [
    "Para decidir el `--max-len` y la posibilidad de utilizar `--sliding-window`, se puede calcular la distribución de longitudes en tokens y elegir el valor del argumento para cubrir el *p90-p95* y obtener un mejor resultado."
   ]
  },
  {
   "cell_type": "code",
   "id": "ae2900843d1dbb0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T19:09:49.090566Z",
     "start_time": "2025-10-21T19:09:44.235171Z"
    }
   },
   "source": [
    "import os\n",
    "# Evita que transformers intente cargar PyTorch/TF/JAX solo para el tokenizador\n",
    "os.environ[\"USE_TORCH\"] = \"0\"\n",
    "os.environ[\"USE_TF\"]    = \"0\"\n",
    "os.environ[\"USE_JAX\"]   = \"0\"\n",
    "\n",
    "from transformers import AutoTokenizer, logging\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\"distilroberta-base\", use_fast=True)\n",
    "df = pd.read_parquet(\"../src/artifacts_rd/prep/merged_reddit.parquet\")\n",
    "lens = df[\"text\"].astype(str).map(lambda s: len(tok(s, truncation=False)[\"input_ids\"]))\n",
    "print(pd.Series(lens).describe(percentiles=[.5, .9, .95, .99]))"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (552 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count   31335.00\n",
      "mean       64.65\n",
      "std        95.41\n",
      "min         3.00\n",
      "50%        36.00\n",
      "90%       145.00\n",
      "95%       211.00\n",
      "99%       440.00\n",
      "max      3412.00\n",
      "Name: text, dtype: float64\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "d790f7a5690dee84",
   "metadata": {},
   "source": [
    "### Argumentos del tokenizador\n",
    "- `--max-len 384`. Como el modelo solo puede leer hasta N tokens al mismo tiempo, se limita a N tokens (384) en la tokenización de cada texto. Si este tiene un nº superior se trunca o se divide en trozos (sliding-window). Elijo 384 tokens máx. por texto por su equilibrio en velocidad y uso de VRAM (no debería de tener problema con mi PC).\n",
    "- `--sliding-window`. Si un texto no cabe en `--max-len`, en lugar de descartar el sobrante se divide en ventanas solapadas, dando como resultado varias filas por cada texto largo.\n",
    "- `--slide-stride 128`. Es el número de tokens que se solapan entre ventanas para no cortar frases/palabras a la mitad cuando se divide un texto entre ventanas. En este caso si el texto se divide en 2 ventanas la distribución sería:\n",
    "    - Ventana 1: tokens 1-384\n",
    "    - Ventana 2: tokens (384-128) 256-639\n",
    "- `--max-windows-per-doc 6`. Limita a un máximo de 6 ventanas por texto, el resto se recorta. Es mejor que descartar el texto completo, y si supera el máx. de ventanas es muy probable que sea debido a mensajes de logs o diffs y no aporte nada al modelo, solo ruido.\n",
    "- `--filter-max-input-tokens`. Como se ve en la celda anterior (porcentajes) hay textos enormes y con este argumento se descartan sus colas limitando a N tokens antes de dividir en ventanas.\n",
    "- `--num-proc 12`. Usa 12 hilos de ejecución para map paralelo (Mi PC tiene 8C/16T) -> Implementar en el tokenizador\n",
    "- `--batch-chunk-size 512`. Indica el tamaño de trozos (grandes) en cada llamada al tokenizador. -> Implementar en el tokenizador\n",
    "\n",
    "#### Recomendaciones\n",
    "- Si ves que sube mucho el nº de ejemplos → baja --max-windows-per-doc a 4.\n",
    "- Si va justo de RAM → --num-proc 8 y/o --batch-chunk-size 256."
   ]
  },
  {
   "cell_type": "code",
   "id": "b048947f75cfae36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T19:16:04.228829Z",
     "start_time": "2025-10-21T19:15:52.828911Z"
    }
   },
   "source": [
    "from pathlib import Path\n",
    "import os, sys, subprocess\n",
    "\n",
    "base = \"../src/data_prep\"  # si ya lo tienes definido arriba, omite esta línea\n",
    "\n",
    "# ---- Cache HF local, compartido con contenedor ----\n",
    "HF_CACHE = \"/home/diego/BERTolto/.cache/huggingface\"\n",
    "Path(HF_CACHE).mkdir(parents=True, exist_ok=True)\n",
    "Path(f\"{HF_CACHE}/hub\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "env = os.environ.copy()\n",
    "env[\"PYTHONIOENCODING\"] = \"utf-8\"\n",
    "env[\"PYTHONUTF8\"] = \"1\"\n",
    "env[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "env[\"RAYON_NUM_THREADS\"] = \"2\"\n",
    "\n",
    "# caches HF para evitar ~/.cache/huggingface (que puede estar a nombre de root)\n",
    "env[\"HF_HOME\"] = HF_CACHE\n",
    "env[\"HUGGINGFACE_HUB_CACHE\"] = HF_CACHE\n",
    "env[\"TRANSFORMERS_CACHE\"] = f\"{HF_CACHE}/hub\"\n",
    "\n",
    "# ---- Sanity check de entradas ----\n",
    "inputs = [\n",
    "    \"../src/artifacts_rd/prep/split_train_reddit.parquet\",\n",
    "    \"../src/artifacts_rd/prep/split_dev_reddit.parquet\",\n",
    "    \"../src/artifacts_rd/prep/split_test_reddit.parquet\",\n",
    "]\n",
    "for p in inputs:\n",
    "    if not Path(p).exists():\n",
    "        raise FileNotFoundError(f\"Falta el archivo: {p}\")\n",
    "\n",
    "# ---- Comando tokenización (ojo al flag con 'windows' en plural) ----\n",
    "cmd = [\n",
    "    sys.executable, f\"{base}/tokenize_hf.py\",\n",
    "    \"--train-parquet\", inputs[0],\n",
    "    \"--dev-parquet\", inputs[1],\n",
    "    \"--test-parquet\", inputs[2],\n",
    "    \"--out-dir\", \"../src/artifacts_rd/hf_distilroberta\",\n",
    "    \"--base-model\", \"distilroberta-base\",\n",
    "    \"--max-len\", \"384\",\n",
    "    \"--sliding-window\",\n",
    "    \"--slide-stride\", \"128\",\n",
    "    \"--max-windows-per-doc\", \"8\",  # <- nombre de flag corregido\n",
    "    \"--filter-max-input-tokens\", \"8192\",\n",
    "    # \"--use-domain-prefix\",                # actívalo cuando mezcles fuentes\n",
    "    \"--num-proc\", \"8\",\n",
    "    \"--batch-chunk-size\", \"512\",\n",
    "]\n",
    "\n",
    "# ---- Ejecuta mostrando logs aunque haya error ----\n",
    "res = subprocess.run(cmd, env=env, capture_output=True, text=True, encoding=\"utf-8\", errors=\"replace\")\n",
    "print(\"=== STDOUT ===\\n\", res.stdout)\n",
    "print(\"=== STDERR ===\\n\", res.stderr)\n",
    "res.check_returncode()  # si hubo error, aquí explota, pero ya viste los logs"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STDOUT ===\n",
      " OK -> ../src/artifacts_rd/hf_distilroberta/dataset ../src/artifacts_rd/hf_distilroberta/tokenizer\n",
      "\n",
      "=== STDERR ===\n",
      " /home/diego/BERTolto/.venv/lib/python3.10/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (476 > 384). Running this sequence through the model will result in indexing errors\n",
      "\n",
      "Map (num_proc=8):   0%|          | 0/16225 [00:00<?, ? examples/s]\n",
      "Map (num_proc=8):   3%|▎         | 512/16225 [00:00<00:22, 705.91 examples/s]\n",
      "Map (num_proc=8):  25%|██▌       | 4096/16225 [00:00<00:01, 6476.75 examples/s]\n",
      "Map (num_proc=8):  50%|█████     | 8192/16225 [00:00<00:00, 12990.27 examples/s]\n",
      "Map (num_proc=8):  88%|████████▊ | 14257/16225 [00:01<00:00, 21347.75 examples/s]\n",
      "Map (num_proc=8): 100%|██████████| 16225/16225 [00:01<00:00, 13400.80 examples/s]\n",
      "\n",
      "Map (num_proc=8):   0%|          | 0/8303 [00:00<?, ? examples/s]\n",
      "Map (num_proc=8):   6%|▌         | 512/8303 [00:00<00:10, 747.97 examples/s]\n",
      "Map (num_proc=8):  43%|████▎     | 3584/8303 [00:00<00:00, 5851.76 examples/s]\n",
      "Map (num_proc=8):  87%|████████▋ | 7251/8303 [00:00<00:00, 11766.40 examples/s]\n",
      "Map (num_proc=8): 100%|██████████| 8303/8303 [00:01<00:00, 7815.52 examples/s] \n",
      "\n",
      "Map (num_proc=8):   0%|          | 0/6807 [00:00<?, ? examples/s]\n",
      "Map (num_proc=8):   8%|▊         | 512/6807 [00:00<00:08, 733.31 examples/s]\n",
      "Map (num_proc=8):  78%|███████▊  | 5279/6807 [00:00<00:00, 8538.00 examples/s]\n",
      "Map (num_proc=8): 100%|██████████| 6807/6807 [00:01<00:00, 6460.65 examples/s]\n",
      "\n",
      "Saving the dataset (0/1 shards):   0%|          | 0/16640 [00:00<?, ? examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 16640/16640 [00:00<00:00, 1151570.26 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 16640/16640 [00:00<00:00, 1142110.30 examples/s]\n",
      "\n",
      "Saving the dataset (0/1 shards):   0%|          | 0/8421 [00:00<?, ? examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 8421/8421 [00:00<00:00, 963480.56 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 8421/8421 [00:00<00:00, 950234.97 examples/s]\n",
      "\n",
      "Saving the dataset (0/1 shards):   0%|          | 0/6917 [00:00<?, ? examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 6917/6917 [00:00<00:00, 971438.16 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 6917/6917 [00:00<00:00, 957112.72 examples/s]\n",
      "\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "993ee413a0d20f9d",
   "metadata": {},
   "source": [
    "**Factor de expansión** -> Si se va demasiado, bajar --max-window a 4"
   ]
  },
  {
   "cell_type": "code",
   "id": "13781e223df865ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T19:27:44.155475Z",
     "start_time": "2025-10-21T19:27:44.056317Z"
    }
   },
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "BASE_ARTI_R = Path(\"../src/artifacts_rd/hf_distilroberta\")\n",
    "ds_r  = load_from_disk(str(BASE_ARTI_R / \"dataset\"))\n",
    "tok_r = AutoTokenizer.from_pretrained(str(BASE_ARTI_R / \"tokenizer\"), use_fast=True)\n",
    "\n",
    "print({k: len(ds_r[k]) for k in ds_r.keys()})\n",
    "print(\"Ejemplo decodificado:\", tok_r.decode(ds_r[\"train\"][0][\"input_ids\"])[:300])\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 16640, 'validation': 8421, 'test': 6917}\n",
      "Ejemplo decodificado: <s>Awesome</s>\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "1b664984b01679aa",
   "metadata": {},
   "source": [
    "**Sanity check** -> Comprobar que no hay basura"
   ]
  },
  {
   "cell_type": "code",
   "id": "784f8f2c419dd617",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T19:27:58.135156Z",
     "start_time": "2025-10-21T19:27:58.052820Z"
    }
   },
   "source": [
    "tokenizer.decode(tok_train[\"input_ids\"][0])[:300]"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtokenizer\u001B[49m\u001B[38;5;241m.\u001B[39mdecode(tok_train[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;241m0\u001B[39m])[:\u001B[38;5;241m300\u001B[39m]\n",
      "\u001B[0;31mNameError\u001B[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "cd307133236cf325",
   "metadata": {},
   "source": [
    "**Velocidad** -> Si el tiempo de ejecución es muy elevado usar --max-len 256 o subir --slide-stride a 192 para menos ventanas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
